---
title: The real option value of agricultural research for development projects
author:
  - name: Benjamin Schiek
    email: b.schiek@cgiar.org
    affiliation: CIAT
address:
  - code: CIAT
    address: Alliance of Bioversity International and CIAT, Km 17 Recta Cali-Palmira, Valle del Cauca, Colombia, CP 763537
abstract: |
  Agricultural research for development scientists are increasingly required to valuate the promising, but uncertain and distant, impacts of their research. Existing approaches to this task are ill-suited to the multi-stage, high risk, far-from-market nature of such research. The resulting frustration has contributed to growing toxicity in donor-researcher relations. To alleviate the tension, here I adapt real options valuation to multi-stage, far-from-market projects. To prepare the ground, first I adapt the key assumption of lognormally distributed project returns to the agricultural research for development context. Second, I argue that the assumption of risk-neutral valuation is inappropriate in far-from-market contexts; and demonstrate the mathematical implications of non-risk-neutrality for real options valuation. Based on this groundwork, I then derive a multi-stage or "$n$-fold", far-from-market real option value model. For the reader's convenience, these main results are prefaced by an introduction to the basics of real options valuation, including an extension to projects with abandonment value. Finally, as an illustrative example, I apply the derived $n$-fold real option valuation model to valuate a real, 4-stage potato research project.
journal: "Research Policy"
date: "`r Sys.Date()`"
bibliography: AR4Drealoptions.bib
header-includes:
  - \numberwithin{equation}{section}
#linenumbers: true
numbersections: true
csl: elsevier-harvard.csl
output: rticles::elsevier_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      dev = c("png", "tiff"),
                      dpi = 300)
library(tidyverse)
library(flextable)
library(patchwork)

label_size <- 2.5
smallLabel_size <- 2
title_size <- 8
subtitle_size <- 7
legendText_size <- 6
axisText_size <- 6
axisTitle_size <- 7
facetTitle_size <- 7

```

# Introduction

Agricultural research for development (AR4D) scientists are under ever increasing pressure from donors to put a defensible monetary value on their proposed projects. However, AR4D projects tend to be (a) high risk, (b) high reward propositions with (c) long time horizons. They are also (d) far-from-market, or even unfold in the midst of market failures, such that project value and/or changes in value may not be deduced or projected based on movements in the prices of commodities or financial securities.

Properties (a)-(d) make it extremely difficult to valuate AR4D projects in a way that is meaningfully different from a wild, if educated, guess. With good reason, then, AR4D researchers generally resent the mounting pressure to quantify promising, but distant and uncertain, impacts [@leeuwis2018reforming]. The resentment is evident across other scientific disciplines as well [@Petsko2011; @Moriarty2008]. AR4D donors, meanwhile, have grown disillusioned with the crudeness of AR4D project appraisals---even to the point of disbelief, and a commensurate global decline in AR4D funding levels [@hurley2016returns; @hurley2014re; @pardey2018shifting]. The frustration of scientists and donors alike is compounded by increasingly austere fiscal restrictions on donors' ability to commit to the long time horizons of agricultural research. The upshot is an atmosphere of "ever growing distrust" between donors and research communities [@leeuwis2018reforming].

Real options valuation [@trigeorgis1993real; @mcgrath2000assessing] presents a potential mechanism by which researchers can 1) provide meaningful, risk-adjusted project appraisals; as well as 2) provide fiscally constrained donors with the flexibility they require to justify long term commitments---by building in an option for the donor to discontinue project funding after a certain amount of time if the project is underperforming. Note that donors effectively have and exercise this option anyway, as AR4D project funding agreements are usually up for renewal/cancelation every 3-5 years. Somewhat unfairly for researchers, default methods of project appraisal fail to price in this option value.
<!-- can 1) provide meaningful, risk-adjusted project appraisals, 2) secure long term commitments from donors, while at the same time 3) giving donors the flexibility they require to maintain solvent fiscal outlooks. -->

The application of real option valuation (ROV) to the AR4D context is complicated by the multi-stage structure of most AR4D projects. The ROV concept was originally created for one stage processes, at the end of which there arises a natural opportunity for the investor to withhold part of the funding if progress up until that point falls below expectations. AR4D projects, on the other hand, unfold in a series of stages, each of which may be viewed as the option to withhold funding of the subsequent stage.
<!-- , if progress up until that point falls below expectations. -->

In the pharmaceutical context, Cassimon et al. [@cassimon2004valuation] extend ROV to multi-stage projects, building on the "compound option" model developed by Geske in the financial context [@geske1979valuation]. Both Geske and Cassimon et al. derive their models from the Black-Scholes partial differentiation equation [@black1973valuation]. Here I present an alternative derivation and formulation of the $n$-fold real option value model based on straightforward integration, which some may find more intuitive and instructive---especially if they are unfamiliar with the financial origins of Geske's model. I then apply the derived model to appraise a real 4-stage AR4D potato project.
<!-- Cassimons et al. coin the phrase "$n$-fold option" to refer to such models.  -->
<!-- (where $n$ is the number of project stages) -->

It is first necessary to preface this derivation with a defense of the key assumption of lognormally distributed random changes in project value, which is widely eschewed as unrealistic or otherwise naive. To further prepare the ground for the main derivation, it is also necessary to redress some confusion in the ROV literature regarding the interpretation of an artifact inherited from financial contexts known as "risk-neutral valuation".
<!-- More specifically, I show that, in far-from-market contexts where the Black-Scholes "no arbitrage" argument does not apply, the Black-Scholes partial differentiation equation -->

Since much of the target audience is probably unfamiliar with the real options literature, I build up to these results incrementally, starting with an introduction to the basic concept and elementary extension to projects with abandonment value. While AR4D is the motivating context for the present work, the $n$-fold ROV model derived here is broadly applicable to any high risk, high reward, multi-stage, far-from-market, real option context.
<!-- * R script included -->

# Real option value basics

## What is real option value?

In the default approach to project funding decisions, the net present value (NPV) of the project's future impacts is compared against the investment required to implement the project. If the NPV (also sometimes referred to as discounted future cash flow) is less than the investment, then the project proposal is rejected. In other words, letting $x(0)$ represent project NPV as evaluated at time $t = 0$ (the start of the project) and $I$ the required investment,

\begin{equation}
x(0) < I \:\: \rightarrow \:\: \text{reject project}
\end{equation}

However, oftentimes the investment $I$ can be decomposed into an upfront sunk cost $S$ required to initiate and sustain project activities, and a subsequent outlay $K$ required a considerable time later, after certain intermediate project goals have been met. The decision criterion can thus more realistically be written as follows.

\begin{equation}
e^{-rT} (E[x(T)] - K) < S \:\: \rightarrow \:\: \text{reject project}
\label{eq:npvIneq1}
\end{equation}

Where $r$ is the discount rate, and $T$ the time at which the subsequent outlay $K$ is required (usually the same as the project time horizon).

If $x(t)$ $(0 < t \leq T)$ follows a geometric Brownian motion (gBm) described by the equation

\begin{equation}
\begin{split}
\Delta x &= x(t + \Delta t) - x(t) \\
&= x(t) m \Delta t + x(t) s \epsilon \sqrt{\Delta t}
\end{split}
\label{eq:gbmEq}
\end{equation}

Where $\epsilon$ is a normally distributed random variable with mean $0$ and variance $1$, such that

\begin{equation}
\begin{split}
m \Delta t &= E \left[\frac{\Delta x(T)}{x(T)} \right] \\
s^2 \Delta t &= Var \left[\frac{\Delta x(T)}{x(T)} \right]
\end{split}
\end{equation}

Then

\begin{equation}
E[x(T)]\bigr|_{t = 0} = e^{m T} x(0)
\label{eq:ExT}
\end{equation}

(See Hull [@hull9thEdition] for details.)

And inequality \ref{eq:npvIneq1} simplifies as follows.

\begin{equation}
e^{(m - r) T} x(0) - e^{-rT} K < S \:\: \rightarrow \:\: \text{reject project}
\label{eq:npvIneq2}
\end{equation}

Moreover, project managers and stakeholders usually have the option to cancel the subsequent outlay $K$ if critical intermediate project goals are not met. More specifically, they can choose to cancel the outlay if the project NPV evaluated at time $T$ is less than the outlay $K$. The decision criterion may thus be expressed even more realistically as follows.

\begin{equation}
e^{-rT} E[\max(x(T) - K, 0)] < S \:\: \rightarrow \:\: \text{reject project}
\label{eq:rovRaw}
\end{equation}

The left hand side of this inequality is often called the "real option value", because it is analogous to the value of a European call option in financial markets. This term was first coined by Myers [@myers1977determinants], and the real options literature has since become vast. See, for example, Trigeorgis [@trigeorgis1993real], Hayes and Garvin [@hayes1982managing], McGrath and MacMillan [@mcgrath2000assessing], and references for introductions to, extensions of, and variations on the subject. For a special focus on research projects as real options, see Doctor, Newton, and Pearson [@doctor2001managing], and Newton, Paxson, and Widdicks [@newton2004real].

## Closed form expression of (1-stage) real option value

For a constant $C$ and a lognormally distributed random variable $q$ such that $\ln(q)$ is normally distributed with mean $\nu$ and variance $\omega^2$, it can be shown through straightforward integration that

\begin{equation}
E[\max(q - C, 0)] = E[q] \Phi \left(\frac{\ln \left(\frac{E[q]}{C}\right) + \omega^2 / 2}{\omega} \right) - C \Phi \left(-\frac{\ln \left(\frac{E[q]}{C}\right) - \omega^2 / 2}{\omega} \right)
\label{eq:part1}
\end{equation}

Where $\Phi(\zeta)$ is the standard normal cumulative distribution function.

\begin{equation}
\Phi(\zeta) = \frac{1}{\sqrt{2 \pi}} \int^{\zeta}_{-\infty} e^{-\frac{\zeta^2}{2}}
\end{equation}

See Appendix for proof.

If $x(t)$ is the gBm described above, then it is lognormally distributed. More specifically, $\ln(x(t))$ is normally distributed with mean $\ln(x(\hat{t})) + (m - s^2 / 2 ) \tau$ and variance $s^2 \tau$, where $\hat{t}$ is the time at which the evaluation is made and $\tau = T - \hat{t}$. Hence, by equation \ref{eq:part1},

\begin{equation}
\begin{split}
E[\max(x(T) - K, 0)]\bigr|_{t = \hat{t}} &= \\
\:\:\: e^{m \tau} x(\hat{t}) \Phi \left(\frac{\ln \left(\frac{x(\hat{t})}{K} \right) + \left(m + \frac{s^2}{2} \right) \tau}{s \sqrt{\tau}} \right) - K \Phi \left(\frac{\ln \left( \frac{x(\hat{t})}{K} \right) + \left(m - \frac{s^2}{2} \right) \tau}{s \sqrt{\tau}} \right)
\end{split}
\end{equation}

Or, more compactly,

\begin{equation}
E[\max(x(T) - K, 0)] \bigr|_{t = \hat{t}} = e^{m \tau} x(\hat{t}) \Phi(\delta + s \sqrt{\tau}) - K \Phi(\delta)
\label{eq:EmaxTK}
\end{equation}

Where

\begin{equation}
\delta = \frac{\ln \left(\frac{x(\hat{t})}{K} \right) + \left(m - \frac{s^2}{2} \right) \tau}{s \sqrt{\tau}}
\end{equation}
<!-- \begin{equation} -->
<!-- E[v(T)] \bigr|_{t = \hat{t}} = v(\hat{t}) e^{\alpha \tau} -->
<!-- \end{equation} -->
<!-- \begin{equation} -->
<!-- Var[\ln(v(T))] \bigr|_{t = \hat{t}} = \beta^2 \tau -->
<!-- \end{equation} -->
<!-- Where $\alpha = 1 / \tau \: E[\Delta v / v]$, and $\beta^2 = 1 / \tau \:Var[\Delta v / v]$. -->
<!-- (See Hull [@hull9thEdition] for details.) -->
<!-- Substituting $v(T)$ for $q$, $E[v(T)]\bigr|_{t = \hat{t}}$ for $E[q]$, and $Var[\ln(v(T))] \bigr|_{t = \hat{t}}$ for $\omega$, -->

Multiplying this through by the discount factor then gives the following closed form expression for real option value (ROV), evaluated at some time $t = \hat{t}$.

\begin{equation}
\begin{split}
ROV &= e^{-r\tau} E[\max(x(T) - K, 0)]\bigr|_{t = \hat{t}} \\
&= e^{(m - r) \tau} x(\hat{t}) \Phi(\delta + s \sqrt{\tau}) - e^{-r \tau} K \Phi(\delta)
\end{split}
\label{eq:rov}
\end{equation}

In the ROV literature, $m$ is usually set equal to $r$ due to an artifact inherited from the financial context called "risk neutral valuation". Farther below, I demonstrate that there is no theoretical or empirical premise for the assumption of risk neutral valuation in far-from-market ROV contexts, and thus no logical premise for setting $m = r$ here.

## Developing real options intuition in the AR4D context

To build intuition, it is instructive to compare graphs of ROV (equation \ref{eq:rov}) and NPV (left-hand side of inequality \ref{eq:npvIneq2}) with respect to the NPV-to-cost ratio $x(0)/K$. In the financial context, this ratio is known as the "moneyness". The option is said to be "in the money" if the ratio is greater than one, "out of the money" if the ratio is less than one, and "at the money" if the ratio equals one. Options with a moneyness ratio well in excess of one are said to be "deep in the money". A generic example is given in the top panel of Figure \ref{fig:rovIllust}. Note that the difference between ROV and NPV shrinks as moneyness is higher. And note that deep in the money ROV does not differ significantly from NPV unless project risk, reflected in the volatility parameter $s$, is sufficiently high.

It is also instructive to keep an eye on the term $\Phi(\delta)$, plotted in the lower panel of Figure \ref{fig:rovIllust}. This is the probability that the option will expire in the money, thereby triggering exercise of the option. It therefore makes perfect sense that $\Phi(\delta)$ approaches one as the moneyness is higher; but note that it approaches one slower as project risk is higher.

```{r Fig1, fig.show = "hold", fig.width = 4, fig.height=4, fig.align="center", fig.cap="\\label{fig:rovIllust}A hypothetical project's real option value and net present value plotted together over a range of moneyness values. The black line marks 0, while the dotted red line marks the sunk cost $S$ beneath which the project is rejected. The multiple ROV plots correspond to different levels of project volatilitty ($s$). The higher the volatility, the greater the difference between ROV and NPV.", echo = FALSE}


OVfun <- function(X0, K, tau, mm, s, r, output = "OV"){
  
  d2 <- (log(X0 / K) + (mm - s^2 / 2) * tau) / (s * sqrt(tau))
  d1 <- d2 + s * sqrt(tau)
  N1 <- pnorm(d1)
  N2 <- pnorm(d2)
  
  OV <- exp((mm - r) * tau) * X0 * N1 - exp(-r * tau) * K * N2

    if(output == "OV"){
    out <- OV
  }
  
  if(output == "N2"){
    out <- N2
  }
  
  if(output == "N1"){
    out <- N1
  }

  return(out)
}
#===========================================================================
mm <- 0.035
r <- mm
tau <- 35
cv <- seq(0.5, 2, length.out = 3)
cvTxt <- c("low", "medium", "high")
s <- round(cv * mm * sqrt(tau), 2)
X0 <- 10
XoK <- seq(0.2, 3, length.out = 50)
#K <- seq(0.1, 3, length.out = 20) * X0
K <- X0 / XoK
NPV <- X0 - exp(-r * tau) * K
ns <- length(cv)
list_df <- list()
for(i in 1:ns){
  this_s <- s[i]
  ROV <- OVfun(X0, K, tau, mm, this_s, r, output = "OV")
  Phi2 <- OVfun(X0, K, tau, mm, this_s, r, output = "N2")
  df <- data.frame(xx = X0 / K, Type = paste("ROV", cvTxt[i], "risk"), Phi2, Value = ROV)
  list_df[[i]] <- df
}
df_npv <- data.frame(xx = X0 / K, Type = "NPV", Phi2 = NA, Value = NPV)
list_df[[ ns + 1]] <- df_npv

df_plot <- as.data.frame(do.call(rbind, list_df))
colnames(df_plot)[1] <- "x(0)/K"
# df_plot1 <- subset(df_plot, cv == cv[ns])
# df_plot1$cv <- NULL
# df_plot1 <- df_plot1 %>% gather(type, Value, ROV:NPV)
# df_plot2 <- subset(df_plot, cv != cv[ns])

  #n <- length(unique(df_plot1$type))
n <- ns + 1
bag_of_colors <- randomcoloR::distinctColorPalette(k = 2 * n)
color_vec <- sample(bag_of_colors, n)

# gg <- ggplot()
# gg <- gg + geom_line(data = df_plot2, aes(x = `x(0)/K`, y = ROV,
#                                          group = cv),
#                      color = color_vec[2], lwd = 1)
# gg <- gg + geom_line(data = df_plot1, aes(x = `x(0)/K`, y = Value,
#                                           group = type, color = type),
#                      lwd = 1)
#----------------------------------------------------------------------------
# ROV vs. NPV plot
gg <- ggplot(df_plot, aes(x = `x(0)/K`, y = Value,
                          group = Type, color = Type))
gg <- gg + geom_line(lwd = 1)
gg <- gg + scale_color_manual(values = color_vec)
#gg <- gg + labs(y = "Value")
gg <- gg + geom_hline(yintercept = 2, color = "red", linetype = "dashed")
gg <- gg + geom_hline(yintercept = 0)
gg <- gg + theme_bw()
gg <- gg + theme(axis.text = element_blank(),
                 axis.title.y = element_text(size = axisTitle_size),
                 axis.title.x = element_blank(),
                 legend.title = element_blank(),
                 legend.position = "top",
                 legend.text = element_text(size = legendText_size))
gg <- gg + guides(color = guide_legend(nrow = 2, byrow = T))
gg1 <- gg
#----------------------------------------------------------------------------
# Phi2 plot
df_plot <- subset(df_plot, Type != "NPV")
gg <- ggplot(df_plot, aes(x = `x(0)/K`, y = Phi2,
                           group = Type, color = Type))
gg <- gg + geom_line(lwd = 1)
gg <- gg + scale_color_manual(values = color_vec[-1])
gg <- gg + labs(y = "Phi 2")
gg <- gg + theme_bw()
gg <- gg + theme(
                 axis.text = element_text(size = axisText_size),
                 axis.title = element_text(size = axisTitle_size),
                 legend.title = element_blank(),
                 legend.position = "none",
                 legend.text = element_text(size = legendText_size))
gg2 <- gg
#----------------------------------------------------------------------------
gg1 + gg2 + plot_layout(ncol = 1, heights = c(1, 1 / 2))

```

The cost $K$ is commonly referred to as the "exercise cost", because it is incurred only in the event that the investor decides to exercise the option. The random variable $x(t)$ is commonly referred to as the "underlying". The term $\Phi(\delta + s \sqrt{\tau})$ is equal to the partial derivative $\partial ROV / \partial x$, and thus provides insight into ROV sensitivity to movements in the underlying. Although not graphed here, it is easy to see that $\Phi(\delta + s \sqrt{\tau})$ will also approach one as the moneyness is higher. This also makes perfect sense. It simply means that, as the probability of exercise grows and ROV becomes indistinguishable from NPV, so likewise do changes in ROV become indistinguishable from changes in the underlying NPV.

Figure \ref{fig:rovIllust} illustrates the importance of risk and reward when deciding whether or not to take an ROV approach to project appraisal. The ROV approach is suitable in the AR4D context because AR4D projects tend to be both high reward (i.e., deep in the money) and high risk. If AR4D projects were only high reward, with little risk, then there would be no point in calculating ROV, as it would not differ significantly from NPV.

## Real options with adandonment value \label{sec:abandVal}

It is easy and worthwhile to extend the ROV formula in equation \ref{eq:rov} to the slightly more general case where the project generates a guaranteed minimum benefit regardless of whether or not the project is successful. In the AR4D context, this guaranteed minimum benefit might correspond to the value of new or upgraded labs and testing facilities, and/or improved human capital through training. Let this guaranteed minimum benefit, also known as the project's abandonment value, be denoted $B$. Then the ROV formula can be generalized to accommodate such cases as follows.

\begin{equation}
ROV = e^{-r T} E[\max(x(T) - K, B)] \:\:;\:\:\: B \geq 0
\label{eq:rovBraw}
\end{equation}

Such that the decision criterion becomes

\begin{equation}
e^{-r T} E[\max(x(T) - K, B)] < S \:\: \rightarrow \:\: \text{reject project}
\label{eq:rovBrawCond}
\end{equation}

But note that, by the general formula for $\max(a, b)$, i.e.,

\begin{equation}
\max(a, b) = \frac{1}{2} (a + b - \left| a - b \right|)
\end{equation}

Equation \ref{eq:rovBraw} can be rewritten

\begin{equation}
ROV = e^{-r T} (E[\max(x(T) - K + B, 0)] + B)
\end{equation}

And hence, by equation \ref{eq:part1},

\begin{equation}
\begin{split}
ROV &= e^{(m - r) \tau} x(\hat{t}) \Phi(\delta + s \sqrt{\tau}) - e^{-r \tau} (K - B) \Phi(\delta) + B \\
&= e^{(m - r) \tau} x(\hat{t}) \Phi(\delta + s \sqrt{\tau}) - e^{-r \tau} K \Phi(\delta) +  e^{-r \tau} B( 1 + \Phi(\delta))
\end{split}
\label{eq:rovB}
\end{equation}

Where

\begin{equation}
\delta = \frac{\ln \left(\frac{x(\hat{t})}{K - B} \right) + \left(m - \frac{s^2}{2} \right) \tau}{s \sqrt{\tau}}
\end{equation}

And where it is easy to see that this reduces to equation \ref{eq:rov} when $B = 0$.

## Quantifying project NPV volatility \label{sec:volEst}

<!-- As noted above, AR4D projects are characterized by high, potentially disruptive, expected net benefits, but also by extreme uncertainty surrounding these expected net benefits. Expected project impacts are vulnerable to a host of both research and non-research related factors (to be discussed in more detail farther below), such that they can easily double or halve over the life of the project. Project appraisals that do not price in, or at least provide some treatment of, this vast uncertainty are thus of limited use to prospective donors. -->
<!-- given new additions to or subtractions from the list of countries targeted for release of the new technology. -->
As noted above, one of the key advantages of ROV is that it provides a mechanism by which to factor uncertainty into project appraisals; and this is achieved through adjustments to the volatility parameter $s$. However, there remains the question of how to calculate or deduce a plausible value for $s$. In the financial context, a direct calculation is possible based on easily accessible historical price time series. In the real options context, direct calculation is generally not possible, since analogous historical project NPV time series data usually do not exist.

In practice, applied ROV studies generally make an educated guess at the volatility parameter, and then conduct sensitivity analysis around the guess (as, for example, in Majd and Pindyck [@majd1987time] or Kemna [@kemna1993case]). While this approach is, in some sense, pragmatic, it is also problematic, because it is difficult to build intuition about what constitutes a plausible value of $s$ for a given project. Sensitivity analysis is one way of building such intuition, but it is a blunt, laborious instrument; and the intuition gained as a result generally does not transfer to other projects. In an alternative, novel approach, Pennings and Lint [@pennings1997option] directly calculate the volatility parameter based on a painstakingly assembled project NPV time series. While clearly more rigorous than the former approach, this is less expedient and probably not possible under most AR4D time, resource, and data constraints.

To find an intermediate point on the rigor-pragmatism spectrum, consider: While it is true that research managers (and/or stakeholders and/or the foresight economists conducting the project appraisal) generally do not have an intuitive grasp of the project NPV volatility parameter itself, they do generally have some idea of the uncertainty surrounding project NPV estimates, such that they can be asked to quantify minimum and maximum bounds on project NPV, in addition to the NPV estimation itself. For example, they can say that NPV is $x(\hat{t})$, but that it might be as much as $100 \%$ greater than $x(\hat{t})$ or as little as $50\%$ lower than $x(\hat{t})$. (Or they may give the same information in terms of magnitude, which is then easily transformed into percentage terms.)

These upper and lower percentage errors may then be interpreted as the upper and lower bounds on the $95\%$ confidence interval about the expected percentage change in project NPV between the start and end of the project. Another phrase for "percentage change" is "arithmetic return" (call this $R$), which may then be converted to a log return ($y$) by the formula $y = \ln(R + 1)$. Assuming $x(t)$ is follows a gBm, then the upper and lower log returns, $\overline{y}$ and $\underline{y}$, may be expressed as follows.

\begin{equation}
\begin{split}
\overline{y} &= \mu + z \sigma \\
\underline{y} &= \mu - z \sigma
\end{split}
\end{equation}

Where $z = 1.96$ is the standard score corresponding to the $95\%$ confidence interval of a normally distributed random variable, and

\begin{equation}
\begin{split}
\mu &= \left(m - \frac{s^2}{2} \right) \tau \\
\sigma &= s^2 \tau
\end{split}
\end{equation}

This system of equations may then be solved for both the project volatility parameter $s$ and mean growth rate $m$ in terms of the upper and lower log returns, the project time horizon $T$, and the standard score $z$, as follows.

\begin{equation}
\begin{split}
s &= \frac{1}{2 \sqrt{\tau} z} (\overline{y} - \underline{y}) \\
m &= \frac{1}{2 \tau} \left( \overline{y} + \underline{y} + \frac{(\overline{y} - \underline{y} )^2}{4 z^2} \right)
\end{split}
\end{equation}
<!-- yMax <- 1.1 -->
<!-- yMin <- -0.6 -->
<!-- # Define confidence interval of interest, usually 95% -->
<!-- zBound <- 2#1.96 -->
<!-- # Back out s and m -->
<!-- s <- (yMax - yMin) / (2 * zBound * sqrt(tauN)) -->
<!-- mu <- (yMax + yMin) / 2 -->
<!-- m <- (mu / tauN + s^2 / 2) -->
<!-- m <- 1 / (2 * tauN) * (yMax + yMin + (yMax - yMin)^2 / (4 * zBound^2)) -->

The coefficient of variation $\frac{s}{m \sqrt{\tau}}$ is a handy, normalized measure of project NPV volatility over time. One should not be surprised to see high coefficients of variation in the AR4D context. Because AR4D projects are usually deep in the money, a low coefficient of variation may indicate that ROV will not differ significantly from NPV, and that there is thus no point in calculating ROV.

Note also that, in the process of deducing $s$ and $m$, the expected project NPV log return $\mu$ is calculated, and that the expected project NPV at future times may be calculated by equation \ref{eq:ExT}. These calculations may serve as further checks on the plausibility of parameter values, project NPV estimates, and project NPV upper and lower bounds.

The standard score $z$ can of course be adjusted to match the practitioner's idea of a suitable confidence interval defined by the bounds $\overline{y}$ and $\underline{y}$. Note that the system of equations above can still be solved for $m$ and $s$ in the absence of an estimate for one of the bounds $\overline{y}$ or $\underline{y}$, if an estimate for the expected log return $\mu$ can be obtained.

## Formalizing the $n$-fold real option value of multi-stage AR4D projects \label{sec:resStages}

<!-- [@mitchell1988managing] -->
AR4D projects usually unfold in a series of stages, the precise structure of which can vary considerably from one project to another. Broadly speaking, there are two types of AR4D project: conventional breeding projects and transgenic projects. The staging of conventional breeding projects must be assessed on a project by project basis, as this varies considerably across crops, technology, and research institution. The emergence of marker assisted breeding further complicates any effort to impose a generic structure on such projects. AR4D transgenic projects, on the other hand, tend to follow the generic four stage process defined below, regardless of crop, technology, and institution. For completeness, a pre-project and post-project stage are also defined, although these typically fall outside of the donor's funding horizon.

* Pre-project: A discovery or "blue skies research" stage, during which new technologies are "discovered" through a careful exploration and extension of existing research---in which serendipity plays a key role---in conjunction with a careful prioritization of research demand. The output of this stage is a new technology proof-of-concept, usually in the form of a novel genotype expressing a set of desired traits, together with a set of recommended farm management practices.

1) A basic replication or scaling up stage, whereby a technology proof-of-concept generated in the discovery phase is reproduced in greenhouse and/or confined field trials, in conjunction with a more refined assessment and prioritization of research demand, especially target populations and ecologies, and the relevant socioeconomic enabling environments (particularly seed systems, government policies, institutions, and markets).

2) A multi-location, multi-season testing stage, whereby successful specimens generated in the preceding stage are taken for further confined field trials under distinct agronomic conditions over multiple cropping seasons.

3) A regulatory dossier stage, wherein detailed agronomic, environmental, and toxicological data, mostly generated during the preceding stages, is compiled into dossier(s) for submission to the National Competent Authorities (NCAs) in the countries targeted for release of the new technology.

4) A deregulation stage, during which the regulatory dossier(s) generated during the previous stage is/are submitted to the NCA(s), which may request further clarification and testing of certain aspects of the proposed technology.

* Post-project: A release and uptake stage, during which the new technology is made available for distribution in the target populations and environments. This stage depends critically on the quality of the socioeconomic enabling environment in the target countries, in conjunction with stewardship from the research institution to ensure correct provision and application of the improved germplasm at the farm level. The holistic "Agricultural Innovation System" view of AR4D considers this stage to be a critical and inseparable part of AR4D projects [@klerkx2010adaptive].

<!-- The duration of project stages varies depending on technology, crop, and research institution; but, as a general rule, transgenic projects (stages 1-4) tend to last 9-11 years. -->
<!-- development of the plant, and the trait assessment, are all known. Thus, none of the costs  -->
<!-- associated with research and development of the gene constructs or of testing them in  -->
<!-- transgenic  events  are  included  (cloning  and  testing  different  R  genes,  testing  the  -->
<!-- durability  of  LB  resistance,  evaluating  different  strategies  for  deployment,  socio- -->
<!-- economic  targeting  studies,  communicating  the  results  to  stakeholders,  and  building  -->
<!-- biotechnology  and  biosafety  facilities).  Also  excluded  are  the  costs  of  building  the  -->
<!-- capacity  of  partners,  scientific  publications,  and  participation  in  scientific  conferences  -->
<!-- apart from any such activities strictly needed for the LBr product development. The costs  -->
<!-- of  obtaining  freedom-to-operate  and/or  intellectual  property  rights  over  the  relevant  -->
<!-- technology are also excluded. It is assumed that these issues and costs have been dealt  -->
<!-- with  at  the  previous  stage  of  the  proof-of-concept  and  has  defined  which  technology  -->
<!-- element will be eventually used for product development. -->
In the default cost-benefit analysis approach to project funding decisions, the decision criterion for multi-stage projects may be formalized as follows.

\begin{equation}
e^{- r T_n} E[x(T_n)] \bigr|_{t = 0} - \Sigma_{i = 1}^{n - 1} e^{-r T_i} K_i < K_n \:\: \rightarrow \:\: \text{reject project}
\label{eq:npvIneqN1}
\end{equation}

Or, if $x(t)$ follows a gBm,

\begin{equation}
e^{(m - r) T_n} x(0) - \Sigma_{i = 1}^{n - 1} e^{-r T_i} K_i < K_n \:\: \rightarrow \:\: \text{reject project}
\label{eq:npvIneqN2}
\end{equation}

This is a straightforward generalization of expression \ref{eq:npvIneq2} to $n$-stage projects.

To formalize the $n$-fold real option value of AR4D projects, it is helpful to first do some relabeling. Let the real option value of a single stage project (equation \ref{eq:rovBraw}) evaluated at some time $\hat{t}$ henceforth be relabeled $f_1(\hat{t})$, with time horizon $T_1$, exercise cost $K_0$, and abandonment value $B_0$. And let $f_0(t) = x(t)$. Then equation \ref{eq:rovBraw} can be written

\begin{equation}
f_1(\hat{t}) = e^{-r \tau_1} E[\max(f_0(T_1) - K_0, B_0)] \bigr|_{t = \hat{t}}
\end{equation}

(Where $\tau_i = T_i - \hat{t}$.)

As explained above, this is the value of the option, but not the obligation, to disburse the funds required to cover final launch and scaling up costs $K_0$ less abandonment value $B_0$ once the research project is finished. Now consider the real option value of a 2-stage project $f_2(\hat{t})$.

\begin{equation}
f_2(\hat{t}) = e^{- r \tau_2} E[\max(f_1(T_2) - K_1, B_1)] \bigr|_{t = \hat{t}}
\end{equation}

This is the value of the option, but not the obligation, to disburse the funds $K_1$ required to implement the second stage of the 2-stage project, once the first stage is finished. Note that $K_1$ corresponds to the sunk cost $S$ in equation \ref{eq:rovBrawCond}. The time horizon $T_2$ refers to the duration of stage 1, and $B_1$ is the abandonment value, if any, generated by the end of stage 1.

Likewise, for a 3-stage project,

\begin{equation}
f_3(\hat{t}) = e^{- r \tau_3} E[\max(f_2(T_3) - K_2, B_2)] \bigr|_{t = \hat{t}}
\end{equation}

Where $T_3$ is the duration of the first stage, $B_2$ is any abandonment value associated with this stage, and $K_2$ the cost of implementing the second stage, of the 3-stage project.

And so on, for any $n$-stage project, the $n$-fold option value $f_n(\hat{t})$ may be defined

\begin{equation}
f_n(\hat{t}) = e^{- r \tau_n} E[\max(f_{n - 1}(T_n) - K_{n - 1}, B_{n - 1})] \bigr|_{t = \hat{t}} \:\:;\:\:\: B_{n - 1} \geq 0
\label{eq:rovNraw}
\end{equation}

In other words, $n$-fold option value is the present value of the option to continue with stage 2 of the project, the value of which is itself the present value of the option to continue with stage 3 of the project, and so forth, up until the present value of the option to continue with stage $n$ of the project, which is itself the present value of the option to implement the new technology---which is itself the project NPV $x(t)$.

The ROV of each stage has as its underlying the ROV of the subsequent stage, which has as its underlying the ROV of the subsequent stage, and so forth, up until the stage 1 ROV, which has as its underlying the project NPV itself. The exercise cost of a given stage is the cost of implementing the subsequent stage.

The formal 1-stage ROV funding decision criterion (inequality \ref{eq:rovBrawCond}) may thus be generalized to $n$-stage projects as follows.

\begin{equation}
e^{-rT_n} E[\max(f_{n - 1}(T_n) - K_{n - 1}, B_{n - 1})] < K_n \:\: \rightarrow \:\: \text{reject project}
\end{equation}

Or, more compactly,

\begin{equation}
f_n < K_n \:\: \rightarrow \:\: \text{reject project}
\label{eq:rovNcond}
\end{equation}

Farther below, I demonstrate that, if $x(t)$ is lognormally distributed, then so is $f_n(t)$. Equation \ref{eq:part1} may therefore be invoked to establish a closed form expression for $f_n(\hat{t})$, if $x(t)$ is lognormally distributed. Before getting to this main result, however, it is first necessary to defend the assumption of lognormal $x(t)$, and to extract far-from-market real options from the thicket of "risk neutral valuation".

# Modeling the evolution of AR4D project NPV

When evaluating AR4D projects as real options, it becomes necessary to think carefully about how project NPV changes over time. This, in turn, requires careful consideration of the causes behind changes in project value. A reasonable starting point in this consideration is the observation that changes in AR4D project value seem to be of two types: research related and non-research related.

Research related changes in project value generally occur at discrete test points, when new information regarding the effectiveness of the new technology becomes available. Non-research related changes in project value occur as a result of changes in the political, socio-economic, and institutional enabling environments where the new technology is to be released. Such changes may include, for example, elections, abrupt changes in government policies, commodity price swings, changes in seed systems and other value chain mechanisms, changes in the security environment, the ebb and flow of public and private sector partnerships to enhance impact, and so forth.

Research related changes occur perhaps 1-4 times per year, while non-research related changes occur with greater frequency, perhaps 1-4 times per quarter. Overall, then, it is reasonable to expect changes in AR4D project value to occur every quarter, as in the left panel of Figure \ref{fig:NPVevol}.

```{r, include=FALSE}

# library(tidyverse)
# library(patchwork)
#----------------------------------------------------------------------------
gbmFun <- function(tau = 40, m = 0.001, s = 0.003, x0 = 1, randVec = NULL) {
  if(is.null(randVec)){randVec <- rnorm(tau)}
  epsilon <- randVec
  lx <- c(); lx[1] <- log(x0)
  drift <- (m - s * s / 2)
  for(t in 2:tau){
      dBt <-  s * epsilon[t]
      lx[t] <- lx[t - 1] + drift + dBt
  }
  x <- exp(lx)
  return(x)
}
# m <- 0.001
# s <- 0.003
# tau <- 400
# x0 <- 1
# x <- gbmFun(tau, m, s, x0, randVec = NULL)
# df_plot <- data.frame(t = 1:tau, x)
# gg <- ggplot(df_plot, aes(x = t, y = x))
# gg <- gg + geom_line()
# acf(diff(log(x)))
# hist(diff(log(x)))
# shapiro.test(diff(log(x)))
#---------------------------------------------------------------------------
# Algorithm 6.2 in Tankov 2003 Financial modeling with jump processes
compoundPoisFun <- function(tau, lambda, m_y = 0, s_y = 1, maxiter = 500){
  N <- rpois(1, lambda * tau)
  U <- runif(N) * tau
  U <- round(U)
  n_iter <- 0; flag <- 1
  while(flag == 1){
    n_iter <- n_iter + 1
      U <- runif(N) * tau
      U <- round(U)
    if(sum(duplicated(U)) > 0 & n_iter <= maxiter){
      flag <- 1
    }else{
      flag <- 0
      if(n_iter == maxiter){
        print("Reached maxiter without generating duplicate-free event times vec. Dropping duplicates.")
        U <- U[-which(duplicated(U))]
      }
    }
  }
  Tt <- U[order(U)]
  J <- exp(rnorm(N, m_y, s_y)) - 1
  #---
  cumJ <- cumsum(J)
  # For explicit modeling of Yt per time step
  Yt <- rep(0, tau)
  Yt[Tt] <- cumJ
  #---
  Tt <- c(0, Tt)
  J <- c(0, J)
  if(Tt[length(Tt)] != tau){
    Tt <- c(Tt, tau)
    J <- c(J, 0)
  }
  cumJ <- cumsum(J)
  df_step <- data.frame(Tt, cumJ)
  #--------------------------
  nEvents <- length(Tt)
  cumYt <- c()
  for(i in 1:(nEvents - 1)){
    tStart <- Tt[i]
    tFin <- Tt[i + 1] - 1
    cumYt[tStart:tFin] <- cumJ[i]
  }
  cumYt[tau] <- cumYt[tFin]
  df_Yt <- data.frame(t = 1:tau, Yt, cumYt)
  
  list_out <- list(df_step, df_Yt)
  return(list_out)

}
#----------------------------------------------------------------------------
FractDim<-function(Data,graphon=FALSE) {
  X=Data;N=length(X);
  jstart=10;jend=floor(10*(log10(N)-1));
  kvec=c(1:4,floor(2^(c(jstart:jend)/4)));
  indkend=length(kvec);
  k=c()
  AvgLmk=c()
  err=c()
  for(indk in 1:indkend)
  {
    k=kvec[indk]
    Xend=c()
    Xsum=c()
    Lmk=c()
    for(m in 1:k)
    {
      Xend=floor((N-m)/k)
      Xsum=sum(abs(X[m+c(1:Xend)*k]-c(0, X[m+c(1:(Xend-1))*k])))
      Lmk[m]=1/k*1/k*(N-1)/Xend*Xsum
    }
    AvgLmk[indk]=mean(Lmk)
    #  err[indk]=sd(log(Lmk))
  }
  x<-log(kvec)
  y<-log(AvgLmk)
  q<-lm(y~x)
  slope<-q$coefficients[2]
  yintcept<-q$coefficients[1]
  yfit<-x*slope+yintcept
  FrDim <- -slope
  avgRes <- mean(abs(q$residuals))
  if(graphon==TRUE)
  {
    plot(x,y,main="If linear then fractal, w/Fr. Dim = (-)slope",xlab="Ln(k)",ylab="Ln(length of curve with interval k)")
    z<-line(x,yfit);abline(coef(z),col='blue');z<-NULL
    #z<-line(x,y);abline(coef(z),col='blue');z<-NULL
  }
  #z<-line(x,y);qq=coef(z)
  #yintcept=qq[1]
  #FrDim=-qq[2]
  return(c(FrDim, avgRes, yintcept))
}
#---------------------------------------------------------------------------
m <- 0.005
s <- m * seq(5.5, 50, length.out = 3)
tau <- 48
x0 <- 1
#rn <- round(runif(1) * 1000)
#rn <- 905
#rn <- 517
# rn <- 340
# set.seed(rn)
# tau <- 12 * 4
lambda <- 0.1


#----------------------------------------------------------------------------
#randVec <- coloredNoise(N = tau, alpha = 1, scaleIt = T)
#acf(randVec)
list_df <- list()
list_spec <- list()
facet_labels <- c()
for(i in 1:length(s)){
  x <- gbmFun(tau, m, s[i], x0, randVec = NULL)
  df_x <- data.frame(t = 1:tau, x, s = as.character(s[i]))
  list_df[[i]] <- df_x
  o <- spectrum(x)
  df_gbmSpec <- data.frame(lfreq = log(o$freq), lpwr = log(o$spec))
  df_gbmSpec <- df_gbmSpec[-1, ]
  df_gbmSpec$s <- as.character(s[i])
  list_spec[[i]] <- df_gbmSpec
  
  mod <- lm(lpwr ~ lfreq, df_gbmSpec)
  # summary(mod)
  alpha <- round(as.numeric(coefficients(mod)[2]), 2)
  # yint <- as.numeric(coefficients(mod)[1])
  # df_out <- as.data.frame(broom::glance(mod))
  # adjR2 <- round(df_out$adj.r.squared, 2)
  # N <- df.residual(mod)
  this_facet_label <- paste0("Slope = ", alpha, " fd = ", round(FractDim(x)[1], 2))
  facet_labels[i] <- this_facet_label
}
#---
df_plot <- as.data.frame(do.call(rbind, list_df))
colnames(df_plot)[1:2] <- c("Time", "Project NPV")
df_plot <- subset(df_plot, s == s[2])
gg <- ggplot(df_plot, aes(x = Time, y = `Project NPV`))
gg <- gg + geom_line()
#gg <- gg + facet_wrap(~s, scales = "free_y", ncol = 1)
gg <- gg + theme_bw()
gg <- gg + theme(axis.text = element_blank(),
                 axis.title = element_text(size = axisTitle_size))
gg_gbm <- gg
#---
df_plot <- as.data.frame(do.call(rbind, list_spec))
colnames(df_plot)[1:2] <- c("Logged frequency", "Logged power spectral density")
df_plot <- subset(df_plot, s == s[2])
#names(facet_labels) <- s
gg <- ggplot(df_plot, aes(x = `Logged frequency`, y = `Logged power spectral density`))
gg <- gg + geom_smooth(method = lm, se = F)
gg <- gg + geom_line()
# gg <- gg + facet_wrap(~s, ncol = 1,
#                       labeller = labeller(s = facet_labels))
gg <- gg + theme_bw()
gg <- gg + theme(axis.text = element_blank(),
                 axis.title = element_text(size = axisTitle_size))
gg_gbmSpec <- gg
#----------------------------------------------------------------------------
list_df <- list()
list_spec <- list()
facet_labels <- c()
for(i in 1:length(s)){
  #s_y = 0.3
  list_out <- compoundPoisFun(tau, lambda, m_y = m, s_y = s[i], maxiter = 500)
  df_x <- list_out[[1]]
  df_x$s <- as.character(s[i])
  list_df[[i]] <- df_x
  
  df_Yt <- list_out[[2]]
  o <- spectrum(df_Yt$cumYt)
  df_spec <- data.frame(lfreq = log(o$freq), lpwr = log(o$spec))
  df_spec <- df_spec[-1, ]
  df_spec$s <- as.character(s[i])
  list_spec[[i]] <- df_spec
  
  mod <- lm(lpwr ~ lfreq, df_spec)
  # summary(mod)
  alpha <- round(as.numeric(coefficients(mod)[2]), 2)
  # yint <- as.numeric(coefficients(mod)[1])
  # df_out <- as.data.frame(broom::glance(mod))
  # adjR2 <- round(df_out$adj.r.squared, 2)
  # N <- df.residual(mod)
  this_facet_label <- paste0("Slope = ", alpha, " fd = ", round(FractDim(df_Yt$cumYt)[1], 2))
  facet_labels[i] <- this_facet_label

}
#---
df_plot <- as.data.frame(do.call(rbind, list_df))
colnames(df_plot)[1:2] <- c("Time", "Project NPV")
df_plot <- subset(df_plot, s == s[2])
gg <- ggplot(df_plot, aes(Time, `Project NPV`))
#gg <- ggplot(df_cpp, aes(t, cumYt))
gg <- gg + geom_step()
#gg <- gg + facet_wrap(~s, ncol = 1)
gg <- gg + theme_bw()
gg <- gg + theme(axis.text = element_blank(),
                 axis.title.y = element_blank(),
                 axis.title.x = element_text(size = axisTitle_size))
gg_cpp <- gg
#---
df_plot <- as.data.frame(do.call(rbind, list_spec))
colnames(df_plot)[1:2] <- c("Logged frequency", "Logged power spectral density")
df_plot <- subset(df_plot, s == s[2])
#names(facet_labels) <- s
gg <- ggplot(df_plot, aes(x = `Logged frequency`, y = `Logged power spectral density`))
gg <- gg + geom_smooth(method = lm, se = F)
gg <- gg + geom_line()
# gg <- gg + facet_wrap(~s, ncol = 1,
#                       labeller = labeller(s = facet_labels))
gg <- gg + theme_bw()
gg <- gg + theme(axis.text = element_blank(),
                 axis.title.y = element_blank(),
                 axis.title.x = element_text(size = axisTitle_size))
gg_cppSpec <- gg

```

```{r Fig2, fig.show = "hold", fig.width = 4, fig.height=2, fig.align="center", fig.cap="\\label{fig:NPVevol}(Left)An example of NPV changing randomly in every time step. (Right)An example of NPV changing randomly at random time steps, and otherwise remaining constant.", echo = FALSE}

gg_gbm + gg_cpp + plot_layout(nrow = 1)


```

This conclusion implies a subtle but fundamental decision to model changes in R&D project value _when they occur_, as opposed to when project managers learn of their occurrence. In the corporate R&D context, Pennings and Lint [@pennings1997option] take the opposite approach, registering changes in project NPV only when management becomes aware of the changes through internal analysis and reporting protocols. In other words, they model changes in project value as "information dam-breaks", whereby new information affecting project value accumulates for a time until it is suddenly released in a report to management, at which time the project value is updated. Such a model of project NPV evolution is depicted in the right panel of Figure \ref{fig:NPVevol}. This approach may be appropriate for corporate R&D, where the value of company assets and activities is ultimately defined by the stock market, and hence (in theory) by publicly available information.
<!-- The information dam-break approach -->
<!-- (Indeed, information dam-breaks are the premise of frontrunning, and hence the bread and butter of many an investment bank.) In the far-from-market R&D context, however, no such nuance comes into play. -->

Whether the evolution of project NPV (or of any stochastic process) resembles the time series on the left or right of Figure \ref{fig:NPVevol} is, to a certain degree, a mere matter of the choice of time step. That is to say, the time series on the left can be transformed into to something resembling the time series on the right merely by plotting over a smaller time step. Likewise, the model on the right can be transformed into the model on the left by plotting over sufficiently large time step. The model on the right becomes unavoidable when said sufficiently large time step results in an unacceptably small number of time steps. In the AR4D context, this is generally not a problem. Projects typically last 9-25 years, such that, given a quarterly time step, the project NPV time series typically contains 36-100 steps.

The time series on the left of Figure \ref{fig:NPVevol} is an example of gBm, while the time series on the right is a compound Poisson process (algorithm 6.2 in Cont and Tankov [@tankov2003financial]). The compound Poisson process (cPp) assumes that changes in value occur at random time intervals. This is fine in the financial context, but may be problematic in real options contexts, where the information affecting project NPV is released through reports to management---events that are usually, at least in the AR4D context, non-random.

## In defense of the gBm model of project NPV

<!-- [Rejection of the gBm model in ROV contexts is partly inherited from the financial context, where gBm is generally viewed as a naive model of price movements. fat tails--debunked by Tankov, jumps--as discussed above, an artifact of the dam-break model, not a concern in ROV contexts, plus jumps can be closely approximated by gBm. en fin, gBm is much more versatile than it is generally given credit for.] -->
Like Pennings and Lint, many real options authors reject the gBm model, preferring instead to select a model which they perceive to more accurately approximate the real time evolution of project NPV. But this comes at a substantial loss of expedience, as the closed form expression in equation \ref{eq:rov} must be replaced by considerably less tractable, less transparent, and less instructive expressions, up to and including numerical methods. Trigeorgis once characterized numerical methods as an unavoidable "bitter pill" that every serious ROV practitioner must come to grips with [@trigeorgis1993real].

It seems safe to say, in hindsight, that energetic ROV proponents like Trigeorgis have overestimated the appetite for bitterness outside of academic audiences. Adoption of ROV thinking by real world decision makers has remained low [@horn2015use; @triantis2005realizing; @driouchi2012real]. ROV critics and proponents alike attribute the tepid reception to the formal complexity of ROV, much of which can be traced back to the use of numerical methods [@triantis2005realizing]. From the perspective of research donors and managers, numerical methods are effectively black boxes; and even ROV experts find themselves bamboozled at times. The much cited numerical exercise by Majd and Pindyck [@majd1987time], for example, contains an elementary error, which was only discovered some twelve years after publication [@milne2000time].
<!-- [---enough so, at any rate, to warrant some revisiting of the matter:] -->
<!-- The case for complex ROV approaches weakens further when considering what is gained in return for sacrificing expedience. Pennings and Lint find only a $2\%$ difference between the the output of their cPp numerical model and that of the gBm closed form model [@pennings1997option]. To what extent is an increase in modeling realism even meaningful in a context where direct measurement of the reality one aspires to approximate---i.e., the evolution of project NPV---is highly problematic? Is the costly increase in realism even relevant to the aims of the modeling exercise? Is there an alternative, less costly way of achieving the same ends? -->
<!-- 33.1 - 32.3 -->

In any methodological decision, there is usually a tradeoff between realism and expedience. The job of the modeler is thus not merely to maximize realism, but to strike the optimal balance between realism and expedience under the particular time and resource constraints of the modeling exercise at hand, and in a way that is clearly relevant to the particular objectives of the exercise. This is especially true in real world decision making contexts, where constraints are considerably more severe than in academic contexts. And the patience of project donors and managers for arcane technical explanations must be counted among the resources that are in short supply.

Before making costly methodological decisions in the name of realism, then, one must carefully consider 1) to what extent an "increase in realism" is even meaningful, i.e., to what extent it is possible to define and measure the reality one aspires to model; 2) whether the proposed increase in realism is actually relevant to the objectives of the modeling exercise, or whether it is just realism for realism's sake; and 3) whether there is a simpler, less costly way to achieve the same increment in realism, or the same modeling objectives which the increment in realism is supposed to serve.
<!-- [One must also be careful of merely replacing one unrealistic artifact with another.]  -->

In the financial context, the reality one aspires to model---i.e. some stochastic financial process, usually a price series---is well defined in the form of historical time series that can be easily downloaded, measured, analyzed, etc. In the far-from-market AR4D context, by contrast, analogous historical time series of project NPV generally do not exist. The reality one aspires to model must be perceived indirectly, based primarily on rational assumptions and logic, as I have just done above. So, in far-from-market real options contexts, it is not even clear what one gets in return for sacrificing expedience.

Then there is the question of the relevance of the increase in realism. In the financial context, the relevance of a realistic model of the time evolution of financial securities is clear: it can make the difference between profit and loss. In far-from-market real options contexts, on the other hand, there is no real need for a good predictor of exactly when or in what order specific changes in project NPV occur. The overarching purpose of modeling project NPV is to calculate its real option value; and for this, the relevant question is rather one of the size distribution of changes. The cPp model might seem more realistic than the gBm model when trying to approximate a time series in which there are a few big changes interspersed by long periods of no change. However, the gBm model can approximate such a time series arbitrarily closely, so long as it is acceptable to substitute "periods of no change" with "periods of negligibly small changes". The periodograms given in Figure \ref{fig:NPVevol2}, which correspond to the time series in the previous Figure, indicate that the size distribution of changes of the two time series is similar, despite their differences in the time domain.

```{r Fig3, fig.show = "hold", fig.width = 4, fig.height=2, fig.align="center", fig.cap="\\label{fig:NPVevol2}(Left) Periodogram of the time series in the left panel of previous figure. (Right) Periodogram of the time series in the right panel of previous figure.", echo = FALSE}

gg_gbmSpec + gg_cppSpec + plot_layout(nrow = 1)

```

This then answers the final question of whether or not a comparable increase in realism, or the objective which the desired increase in realism is supposed to serve, may be achieved at lower cost. In the case of cPp, the graphic above  suggests that yes, the same realism may be achieved, at no cost of expedience, by using the gBm model. This closeness is reflected, for example, in the results of Pennings and Lint, who find only a $2\%$ difference between ROV as calculated by their cPp-based numerical model and ROV as calculated by the gBm-based closed form model [@pennings1997option].
<!-- Given all the research and non-research factors affecting project NPV, it is highly unlikely that there is not at least a small, if negligible, perturbation in NPV in every time step, in which case the gBm model is the more realistic choice (when modeling project NPV evolution as it happens, as opposed to the information dam-break approach). -->
<!-- The question is not when changes in NPV occur, but rather how often do substantial changes to NPV occur? In more technical terms, what is the size distribution of changes? In order to answer this question, it is more instructive to look at the signal's periodogram, not its evolution in the time domain. This can be examined by looking at a periodogram. Model 1 can effectively approximate model 2 to an arbitrary degree of precision by tuning the uncertainty parameter $s$ (Figure \ref)..... A few substantial changes followed by relatively long periods of little change. [The Poisson jump model represents a process in which there are a few substantial changes interspersed among periods of no changes at all. While the gbm model cannot replicate periods of no change exactly, it can approximate such periods arbitrarily closely through adjustments to the volatility parameter.... And recall that it is highly unlikely that there are no changes in project value in any given time step, but rather that there may be long periods of very small changes punctuated by brief periods of large changes. This is perhaps best illustrated by looking at periodograms (Figure ...).] -->
<!-- Geometric Brownian motion is a much more versatile model than portrayed in the literature.... "bitter pill" [trigergis]. criticism fat tails etc. starting with Mandelbrot []. but this has led to misconception... [Tankov]. The fact is that gbm remains a highly versatile model capable of representing a wide variety of stochastic processes by adjustments to the volatility parameters. The key question that Pennings and Lint address with their jump model may be formulated as follows: what is the size distribution of changes in project value?  [P&L Poisson jump model output differed from the lognormal assumption output by just x% [P&L].] -->
<!-- [For the purposes of evaluating real option value, the question is not so much when exactly the changes occur, but rather their size distribution. In other words not the time domain but the frequency domain that is important.] -->
<!-- as compared to the default NPV approach -->
<!-- ## Low adoption of real options thinking due to complexity -->
<!-- Despite a flood of academic interest in ROV following Myers' initial insight, adoption of the real options approach by real world decision makers remains low [@horn2015use; @triantis2005realizing; @driouchi2012real]. ROV critics and proponents alike attribute this tepid reception to the formal complexity of evaluating and explaining ROV as compared to the default NPV approach [@triantis2005realizing]. -->
<!-- Much of this complexity can be traced back to two sources. Firstly, there is the frequent and puzzling assumption of risk neutral valuation in far-from-market real options contexts, just mentioned above. Secondly, many authors reject the key assumption that project NPV evolution may be modeled as a gBm, preferring instead to evaluate equation \ref{eq:rovRaw} by numerical methods that are considerably less tractable, transparent, and instructive than equation \ref{eq:rov}. Trigeorgis, for example, calls numerical methods a "bitter pill" that every serious ROV practitioner must come to grips with [@trigeorgis1993real]. -->
<!-- Before deriving the $n$-fold ROV model, it is first necessary to redress these two sources of confusion in detail. It seems safe to say, in hindsight, that energetic ROV proponents like Trigeorgis may have overestimated the appetite for bitterness outside of academic audiences. After this introductory section, I preface the derivation of the $n$-fold ROV model with a defense of the gBm model of project NPV, followed by a repudiation of risk neutral valuation in far-from-market real options contexts. -->
<!--   present arguments in defense of gBm as a model of far-from-market project NPV. In particular, I note that the a as a much more versatile model than it is given credit for. -->
<!-- Secondly, there is confusion regarding the interpretation, in real options contexts, of the financial artifact known as "risk-neutral valuation". -->
<!-- (see, for example, Hayes and Garvin [@hayes1982managing], McGrath and MacMillan [@mcgrath2000assessing], Doctor, Newton, and Pearson [@doctor2001managing], and Newton, Paxson, and Widdicks [@newton2004real]), -->
<!-- [However, the question of where ... must be assessed on a case by case basis. is a matter of preference. In academic contexts, there is a premium on rigor and realism. Most real options settings, on the other hand, time, resources, patience, and attention-span are in relatively short supply ... there is a premium on transparency, expediency. Pennings and Lint use equation \ref{eq:rov} and find that the numerical method output differs by just x%. That is a lot of extra work for a negligible difference. Whether or not changes occur in every time step is a matter of judicious choice of time step. In most real options settings, changes might not occur every day or every week or even every month, but probably do occur at least once every quarter. Moreover, such meticulous realism quickly lands the practitioner in other problems. In most real options settings, the research, analysis, and reporting protocols by which new information affecting project NPV becomes available occur at predetermined, non-random intervals, whereas the CP model is only valid for events occurring at random intervals. The order of changes in NPV is irrelevant. It is the size distribution of changes that matters, not when they occur. The frequency domain is what matters, not the time evolution.] -->
<!-- # It's ok to be lognormal -->
<!-- When taking a real options approach to project evaluation, it becomes necessary to think carefully about the evolution of project NPV over the life of the project. -->
<!-- Many consider the assumption of lognormally distributed NPV unrealistic. This is to some degree rooted in the original financial context, where the assumption of lognormal security returns is widely viewed as naive... -->
<!-- Bibby and Sorensen [@bibby1996hyperbolic] -->
<!-- Pennings and Lint []. .  misconception [Tankov]. -->

# Risk non-neutrality in far-from-market real options contexts

Another major source of complexity resulting in low adoption of ROV thinking regards the confused interpretation, in real options contexts, of the financial artifact known as "risk-neutral valuation".

The derivation of equation \ref{eq:rov} in the Appendix via the method of straightforward integration is atypical of the ROV literature. Most authors instead cite the Black-Scholes partial differentiation equation [@black1973valuation] as the source of the ROV formula. The method of straightforward integration is followed here because it is a considerably simpler method, and is stripped of financial trappings. In financial contexts, the Black-Scholes approach is advantageous because it generates a whole class of functional forms known as the "financial derivatives", of which the European call option formula (the financial analogue to equation \ref{eq:rov}) is just one.

More importantly, the Black-Scholes approach reveals, as a by-product, the deep result known as the principle of risk-neutral valuation: If it is not possible to make risk-free profits above the risk-free rate of return (the "no-arbitrage" rule), then investors are risk-neutral, i.e. "investors do not increase the expected return they require from an investment in order to compensate for increased risk" [@hull9thEdition]. Mathematically, this means that the $m$ in equation \ref{eq:rov} must be set equal to $r$ (where $r$, in financial contexts, refers to the risk-free rate of return).

The principle of risk-neutral valuation rests squarely upon the no-arbitrage rule, which is enforced through the market. This is fine in the financial context. However, in real options contexts, there is no clear theoretical or empirical basis for the no-arbitrage rule. Real project NPV is not a traded good, and there is no clear mechanism that might serve as a market analogue. On the contrary, most ROV contexts, especially research contexts, may be characterized as far-from-market---or even market failures, which the underlying project is supposed to redress. Empirically, it is a matter of public knowledge that project donors and managers are generally not risk-neutral; i.e., they would require an increase in expected project NPV to justify funding for a project with increased risk.

Critics note that the ROV literature is silent and/or conflicted on this point [@borison2005real; @block2007real]. It is not uncommon for ROV studies, and even ROV introductory texts, to assume risk-neutral valuation without any justification (see, for example, Trigeorgis [@trigeorgis1993real], Majd and Pindyck [@majd1987time], or Kemna [@kemna1993case]). Some studies acknowledge the invalidity of the no-arbitrage argument in ROV contexts, but instead invoke "complete markets" to justify risk-neutral valuation (see, for example, Pennings and Lint [@pennings1997option]). However, the complete markets assumption implies that 1) project NPV can be simulated by a portfolio of traded securities, and hence project risk can be hedged away by buying and/or selling these traded securities; and 2) project managers actually engage in the buying and selling of securities necessary to achieve this hedge. One may say without controversy that neither (1) nor (2) are common features of most far-from-market project management landscapes.

Mathematically, the absence of any empirical or theoretical premise for risk-neutral valuation in far-from-market real options contexts means that there is no reason to set $m$ equal to $r$.

## The far-from-market Black-Scholes PDE

To see this, recall that, if $x(t)$ is a gBm, then, by Ito's lemma, the evolution of a function $f(x, t)$ is described as follows.

\begin{equation}
\Delta f = \left( \frac{\partial f}{\partial x} x m + \frac{\partial f}{\partial t} + \frac{1}{2} \frac{\partial^2 f}{\partial x^2} x^2 s^2 \right) \Delta t + \frac{\partial f}{\partial x} x s \epsilon \sqrt{\Delta t}
\label{eq:itoLem}
\end{equation}

(See Hull [@hull9thEdition] for details.)

Black and Scholes [@black1973valuation] famously noted that equations \ref{eq:gbmEq} and \ref{eq:itoLem} could be combined so as to eliminate the random term as follows.

\begin{equation}
\Delta f - \frac{\partial f}{\partial x} \Delta x = \left(\frac{\partial f}{\partial t} + \frac{1}{2} \frac{\partial^2 f}{\partial x^2} x^2 s^2 \right) \Delta t
\label{eq:bsInsight1}
\end{equation}

In the financial context, the left-hand side of this equation may be thought of as the instantaneous evolution over the increment $\Delta t$ of a portfolio long one share of the financial derivative $f$ and short a quantity $\partial f / \partial x$ of the underlying security $x(t)$. This is where Black and Scholes applied their no-arbitrage argument: Since the random---i.e. risky---term has been eliminated from equation \ref{eq:bsInsight1}, then the profit or loss of this portfolio over the increment $\Delta t$ must be riskless. That is, it must be equal to the starting value of the portfolio times the risk free rate ($r$).

\begin{equation}
\Delta f - \frac{\partial f}{\partial x} \Delta x = \left( f - \frac{\partial f}{\partial x} x \right) r \Delta t
\end{equation}

Equating the right-hand side of this equation with the right-hand side of the previous equation, the $\Delta t$'s cancel, resulting in the Black-Scholes partial differentiation equation (PDE).

\begin{equation}
\left( f - \frac{\partial f}{\partial x} x \right) r = \frac{\partial f}{\partial t} + \frac{1}{2} \frac{\partial^2 f}{\partial x^2} x^2 s^2 \end{equation}

Black and Scholes solved this PDE under the boundary condition $f(T) = \max(x(T) - K, 0)$, resulting in an expression for $f$ identical to the one derived above in equation \ref{eq:EmaxTK}, with the exception that $m$ is replaced with $r$.

But Black and Scholes' insight can be decomposed into two consecutive insights. The first insight is that, by eliminating the random terms in equation \ref{eq:bsInsight1}, the resulting expression is deterministic. And it equates a composite evolution in terms of $\Delta f$ and $\Delta x$ on the left-hand side to a time evolution $\Delta t$ on the right-hand side. Regardless of the no-arbitrage rule, it follows trivially that

\begin{equation}
\Delta f - \frac{\partial f}{\partial x} \Delta x = \left( f - \frac{\partial f}{\partial x} x \right) \kappa \Delta t
\end{equation}

<!-- So long as -->
<!-- \begin{equation} -->
<!-- \kappa = \frac{\frac{\partial f}{\partial t} + \frac{1}{2} \frac{\partial^2 f}{\partial x^2} s^2 x^2}{f - \frac{\partial f}{\partial x} x} -->
<!-- \end{equation} -->
<!-- holds. -->
Where $\kappa$ is constant with respect to the interval $\Delta t$.

The second insight is about resolving the value of $\kappa$. In financial markets, the no-arbitrage rule requires that $\kappa = r$. In the absence of the no-arbitrage rule, however, the $r$ in the Black-Scholes PDE must be replaced by $\kappa$. Solving this non-market version of the Black-Scholes PDE at the boundary condition $f(T) = \max(x(T) - K, 0)$ and comparing it to equation \ref{eq:EmaxTK}, which is obtained through straightforward integration, reveals that $\kappa$ must default to $m$. In the absence of the no-arbitrage rule, then, the Black-Scholes PDE must default to

\begin{equation}
\left( f - \frac{\partial f}{\partial x} x \right) m = \frac{\partial f}{\partial t} + \frac{1}{2} \frac{\partial^2 f}{\partial x^2} s^2 x^2
\label{eq:ffmBSpde}
\end{equation}

This is the relevant version of the Black-Scholes PDE in far-from-market real options contexts.
<!-- the now deterministic equation means that the evolution in terms of $\Delta f$ and $\Delta x$ on the left-hand side of equation \ref{eq:bsInsight1} may be rewritten in terms of a time evolution $\Delta t$. That is, we have an equation of the following form. -->
<!-- the evolution of the left-hand side must be constant over the interval $\Delta t$. This means -->
<!-- \begin{equation} -->
<!-- a \Delta f - b \Delta x = c \Delta t -->
<!-- \end{equation} -->
<!-- [Where $a$ ...] -->
<!-- Regardless of the no-arbitrage argument, it follows trivially that -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- a f \kappa \Delta t - b x \kappa \Delta t  &= c \Delta t \\ -->
<!-- (a f - b x) \kappa \Delta t  &= c \Delta t -->
<!-- \end{split} -->
<!-- \end{equation} -->
<!-- And hence, trivially, -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- a f \kappa \Delta t - b x \kappa \Delta t  &= c \Delta t \\ -->
<!-- (a f - b x) \kappa \Delta t  &= c \Delta t -->
<!-- \end{split} -->
<!-- \end{equation} -->

# A closed form expression for $n$-fold real option value

Now we are in a position to derive the $n$-fold real option value formula. To begin, consider the far-from-market Black-Scholes PDE (equation \ref{eq:ffmBSpde}) for the 1-fold real option value function $f_1$, rearranged as an expression for $m f_1$.

<!-- The derivation consists of two steps. First we show that the $n$-fold real option value $f_n$ is lognormally distributed if project NPV $x(t)$ is lognormally distributed. Then, we show A closed form expression for $f_n$ then follows from equation \ref{eq:part1}. -->

\begin{equation}
m f_1 = \frac{\partial f_1}{\partial x} x m + \frac{\partial f_1}{\partial t} + \frac{s^2 x^2}{2} \frac{\partial^2 f_1}{\partial x^2}
\end{equation}

And consider Ito's lemma for $f_1$.

\begin{equation}
\Delta f_1 = \left( \frac{\partial f_1}{\partial x} m x + \frac{\partial f_1}{\partial t} + \frac{1}{2} \frac{\partial^2 f_1}{\partial x^2} x^2 s^2 \right) \Delta t + \frac{\partial f_1}{\partial x} x s \epsilon \sqrt{\Delta t}
\end{equation}

Note that the right-hand side of the previous expression is equal to the first term in parentheses on the right-hand side of Ito's lemma. Ito's lemma for $f_1$ may thus be rewritten as follows.

\begin{equation}
\Delta f_1 = m f_1 \Delta t + x \frac{\partial f_1}{\partial x} s \epsilon \sqrt{\Delta t}
\label{eq:itoLemf1}
\end{equation}

The second term on the right-hand side can also be rewritten as follows.

\begin{equation}
x \frac{\partial f_1}{\partial x} s \epsilon \sqrt{\Delta t} = f_1 \eta_{1, 0} s \epsilon \sqrt{\Delta t}
\end{equation}

Where $\eta_{1, 0}$ is the elasticity of $f_1$ with respect to the project NPV $x$. That is, defining $f_0 = x$,

\begin{equation}
\begin{split}
\eta_{n, 0} &= \frac{f_0}{f_n} \frac{\partial f_n}{\partial f_0} = \frac{\partial \ln(f_n)}{\partial \ln(f_0)} \\
&= \frac{1}{100} \frac{\% \Delta f_n}{\% \Delta f_0}
\end{split}
\end{equation}

Equation \ref{eq:itoLemf1} can thus be rewritten

\begin{equation}
\Delta f_1 = m f_1 \Delta t + f_1 \eta_{1, 0} s \epsilon \sqrt{\Delta t}
\label{eq:gbmf1}
\end{equation}

By which it follows that $\Delta f_1 / f_1$ is normally distributed with mean $m \Delta t$ and variance $s^2 \eta_{1, 0}^2 \Delta t$. From this, it follows that $f_1$ is a geometric Brownian movement, such that $\ln(f_1)$ is normally distributed with mean $\ln(f_1(\hat{t})) + \left(m - \frac{s^2 \eta_{1, 0}^2}{2} \right) \tau_1$ and variance $s^2 \eta_{1, 0}^2 \tau_1$.

It follows that a 2-fold real option value function $f_2(f_1, t)$ also has a far-from-market Black-Scholes PDE.

\begin{equation}
\left( f_2 - \frac{\partial f_2}{\partial f_1} f_1 \right) m = \frac{\partial f_2}{\partial t} + \frac{1}{2} \frac{\partial^2 f_2}{\partial f_1^2} s^2 f_1^2
\end{equation}

With corresponding Ito's lemma

\begin{equation}
\Delta f_2 = \left( \frac{\partial f_2}{\partial f_1} m f_1 + \frac{\partial f_2{\partial t} + \frac{1}{2} \frac{\partial^2 f_2}{\partial f_1^2} s^2 f_1^2 \right) \Delta t + \frac{\partial f_2}{\partial f_1} f_1 s \epsilon \sqrt{\Delta t}
\end{equation}

Such that

\begin{equation}
\begin{split}
\Delta f_2 &= m f_2 \Delta t + f_2 \eta_{1, 0}  \frac{\partial f_2}{\partial f_1} s \epsilon \sqrt{\Delta t} \\
&= m f_2 \Delta t + f_2 \eta_{2, 0} s \epsilon \sqrt{\Delta t}
\end{split}
\label{eq:gbmf2}
\end{equation}

By which it follows that $\Delta f_2 / f_2$ is normally distributed with mean $m \Delta t$ and variance $s^2 \eta_{2, 0}^2 \Delta t$; and that $\ln(f_2)$ is normally distributed with mean $\ln(f_2(\hat{t})) + \left(m - \frac{s^2 \eta_{2, 0}^2}{2} \right) \tau_2$ and variance $s^2 \eta_{2, 0}^2 \tau_2$. That is, $f_2$ is also a geometric Brownian movement.

And so on for $f_3$, $f_4$, $\dots$, $f_n$, there exists a far-from-market Black-Scholes PDE

