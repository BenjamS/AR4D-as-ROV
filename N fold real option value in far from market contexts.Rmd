---
title: The real option value of agricultural research for development projects
author:
  - name: Benjamin Schiek
    email: b.schiek@cgiar.org
    affiliation: CIAT
address:
  - code: CIAT
    address: Alliance of Bioversity International and CIAT, Km 17 Recta Cali-Palmira, Valle del Cauca, Colombia, CP 763537
abstract: |
  Agricultural research for development projects typically consist of three or four stages, each of which may be thought of as an option on the subsequent stage. level, it must be valued as a compound option. I begin with defense of gbm assumption and address the invaidity of the risk-neutral valuation assumption in the context of agricultural research.
  
journal: "Research Policy"
date: "`r Sys.Date()`"
bibliography: AR4Drealoptions.bib
header-includes:
  - \numberwithin{equation}{section}
#linenumbers: true
#numbersections: true
csl: elsevier-harvard.csl
output: rticles::elsevier_article
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      dev = c("png", "tiff"),
                      dpi = 300)
library(tidyverse)
library(flextable)
library(patchwork)

label_size <- 2.5
smallLabel_size <- 2
title_size <- 8
subtitle_size <- 7
legendText_size <- 7
axisText_size <- 6
axisTitle_size <- 7
facetTitle_size <- 7

```

# Introduction

Agricultural research for development (AR4D) scientists are under ever increasing pressure from donors to put a defensible monetary value on their proposed projects. However, AR4D projects tend to be (a) high risk, high reward propositions with (b) long time horizons. They are also (c) far-from-market, or even unfold in the midst of market failures, such that project value and/or changes in value may not be deduced from movements in the prices of commodities or financial securities. Properties (a)-(c) make it extremely difficult to valuate AR4D projects in a way that is meaningfully different from a wild, if educated, guess.

There is mounting evidence, meanwhile, that donors have grown disillusioned with the crudeness of AR4D project appraisals---even to the point of disbelief, and a commensurate global decline in AR4D funding levels [@hurley2016returns]. The frustration of scientists and donors alike is compounded by increasingly austere fiscal restrictions on donors' ability to commit to the long time horizons of agricultural research. All of this has contributed to an atmosphere of "ever growing distrust" between donors and research communities [@leeuwis2018reforming].

But AR4D projects are also (d) structured in a series of stages, such that, at the end of each stage, there arises a natural opportunity to cancel the remainder of the project if progress up until that point falls below expectations. It stands to reason that fiscally constrained donors could justify long term commitments if given this prerogative. Donors effectively have and exercise this prerogative anyway, as AR4D project funding agreements are typically up for renewal/cancelation every 3-5 years. Another word for this prerogative is "option value". Somewhat unfairly for researchers, default methods of project appraisal do not price in donor option value.

Real options valuation presents a precise, rigorous way to price in donor prerogative to project appraisals. It also accounts for the high uncertainty (i.e. risk) surrounding expected project impacts. Real options valuation thus presents a potential mechanism by which researchers can 1) provide meaningful, risk-adjusted project appraisals, 2) secure long term commitments from donors (contingent upon successful completion of each stage of research), while at the same time 3) formalizing the flexibility donors require to maintain solvent fiscal outlooks.

The vast real options value literature focuses primarily on single stage projects. Cassimon et al. [-@cassimon2004valuation] extend the method to multi-stage projects in the pharmaceutical context, building on the "compound option" model developed by Geske in the financial context [-@geske1979valuation]. Cassimons et al. coin the phrase "$n$-fold option" to refer to such models. Both Geske and Cassimon et al. derive their models from the Black-Scholes partial differentiation equation [-@black1973valuation]. Here I present an alternative derivation and formulation of the $n$-fold real option value model based on straightforward integration, which some may find more intuitive and instructive---especially if they are unfamiliar with the financial origins of Geske's model. I then apply the derived model to appraise a real $4$-stage AR4D potato project.
<!-- (where $n$ is the number of project stages) -->

It is first necessary to preface this derivation with a defense of the key assumption of lognormally distributed random changes in project value, which is widely eschewed in both the financial and real options literature as unrealistic or otherwise naive. To further prepare the ground for the main derivation, it is also necessary to redress some confusion in the real options literature regarding the interpretation of an artifact inherited from financial contexts known as "risk-neutral valuation".
<!-- More specifically, I show that, in far-from-market contexts where the Black-Scholes "no arbitrage" argument does not apply, the Black-Scholes partial differentiation equation -->

Since much of the target audience is probably unfamiliar with the real options literature, I build up to these results incrementally, starting with an introduction to the basic concept. While AR4D is the motivating context for the present work, the $n$-fold model derived here is broadly applicable to any high risk, high reward, multi-stage, far-from-market real option context.
<!-- * R script included -->

# Literature review

## What is real option value?

In the default approach to project funding decisions, the net present value (NPV) of the project's future impacts is compared against the investment required to implement the project. If the NPV (also sometimes referred to as discounted future cash flow) is less than the investment, then the project proposal is rejected. In other words, letting $x(0)$ represent project NPV as evaluated at time $t = 0$ (the start of the project) and $I$ the required investment,

\begin{equation}
x(0) < I \:\: \rightarrow \:\: \text{reject project}
\end{equation}

However, oftentimes the investment $I$ can be decomposed into an upfront sunk cost $S$ required to initiate and sustain project activities, and a subsequent outlay $K$ required a considerable time later, after certain intermediate project goals have been met. The decision criterion can thus more realistically be written as follows.

\begin{equation}
e^{-rT} (E[x(T)] - K) < S \:\: \rightarrow \:\: \text{reject project}
\label{eq:npvIneq1}
\end{equation}

Where $r$ is the discount rate, and $T$ the time at which the subsequent outlay $K$ is required (usually the same as the project time horizon).

If $x(t)$ $(0 < t \leq T)$ follows a geometric Brownian motion (gBm) described by the equation

\begin{equation}
\begin{split}
\Delta x &= x(t + \Delta t) - x(t) \\
&= m x(t) \Delta t + s x(t) \epsilon \sqrt{\Delta t}
\end{split}
\label{eq:gbmEq}
\end{equation}

Where $\epsilon$ is a normally distributed random variable with mean $0$ and variance $1$, such that

\begin{equation}
\begin{split}
m \tau &= E[\frac{\Delta x(T)}{x(T)}] \\
s^2 \tau &= Var[\frac{\Delta x(T)}{x(T)}]
\end{split}
\end{equation}

Then

\begin{equation}
E[x(T)]\bigr|_{t = 0} = e^{m T} x(0)
\label{eq:ExT}
\end{equation}

(See Hull [-@hull9thEdition] for details)

And inequality \ref{eq:npvIneq1} simplifies as follows.

\begin{equation}
e^{(m - r) T} x(0) - e^{-rT} K < S \:\: \rightarrow \:\: \text{reject project}
\label{eq:npvIneq2}
\end{equation}

Moreover, project managers and stakeholders usually have the option to cancel the subsequent outlay $K$ if critical intermediate project goals are not met. More specifically, they can choose to cancel the outlay if the project NPV evaluated at time $T$ is less than the outlay $K$. The decision criterion may thus be expressed even more realistically as follows.

\begin{equation}
e^{-rT} E[max(x(T) - K, 0)] < S \:\: \rightarrow \:\: \text{reject project}
\label{eq:rovRaw}
\end{equation}

The left hand side of this inequality is often called the "real option value", because it is analogous to the value of a European call option in financial markets. This term was first coined by Myers [-@myers1977determinants], and the real options literature has since become vast. See, for example, Trigeorgis [-@trigeorgis1993real], Hayes and Garvin [-@hayes1982managing], McGrath and MacMillan [-@mcgrath2000assessing], and references for introductions to, extensions of, and variations on the subject. For a special focus on research projects as real options, see Doctor, Newton, and Pearson [-@doctor2001managing], and Newton, Paxson, and Widdicks [-@newton2004real].

## Closed form expression of (1-stage) real option value

For a constant $C$ and a lognormally distributed random variable $q$ such that $\ln(q)$ is normally distributed with mean $\nu$ and variance $\omega^2$, it can be shown through straightforward integration that

\begin{equation}
E[max(q - C, 0)] = E[q] \Phi \left(\frac{\ln \left(\fraq{E[q]}{C}\right) + \omega^2 / 2}{\omega} \right) - C \Phi \left(-\frac{\ln \left(\frac{E[q]}{C}\right) - \omega^2 / 2}{\omega} \right)
\label{eq:part1}
\end{equation}

Where $\Phi(\zeta)$ is the standard normal cumulative distribution function.

\begin{equation}
\Phi(\zeta) = \frac{1}{\sqrt{2 \pi}} \int^{\zeta}_{-\infty} e^{-\frac{\zeta^2}{2}}
\end{equation}

See Appendix for proof.

If $x(t)$ is the gBm described above, then it is lognormally distributed. More specifically, $\ln(x(t))$ is normally distributed with mean $\ln(x(\hat{t})) + (m - s^2 / 2 ) \tau$ and variance $s^2 \tau$, where $\hat{t}$ is the time at which the evaluation is made and $\tau = T - \hat{t}$. Hence, by equation \ref{eq:part1},

\begin{equation}
E[max(x(T) - K, 0)]\bigr|_{t = \hat{t}} = e^{m \tau} x(\hat{t}) \Phi \left(\frac{\ln \left(\frac{x(\hat{t})}{K} \right) + \left(m + \frac{s^2}{2} \right) \tau}{s \sqrt{\tau}} \right) - C \Phi \left(\frac{\ln \left( \frac{v(\hat{t})}{K} \right) + \left(m - \frac{s^2}{2} \right) \tau}{s \sqrt{\tau}} \right)
\end{equation}

Or, more compactly,

\begin{equation}
E[max(x(T) - K, 0)] \bigr|_{t = \hat{t}} = e^{m \tau} x(\hat{t}) \Phi(\delta + s \sqrt{\tau}) - K \Phi(\delta)
\label{eq:EmaxTK}
\end{equation}

Where

\begin{equation}
\delta = \frac{\ln \left(\frac{x(\hat{t})}{K} \right) + \left(m - \frac{s^2}{2} \right) \tau}{s \sqrt{\tau}}
\end{equation}
<!-- \begin{equation} -->
<!-- E[v(T)] \bigr|_{t = \hat{t}} = v(\hat{t}) e^{\alpha \tau} -->
<!-- \end{equation} -->
<!-- \begin{equation} -->
<!-- Var[\ln(v(T))] \bigr|_{t = \hat{t}} = \beta^2 \tau -->
<!-- \end{equation} -->
<!-- Where $\alpha = 1 / \tau \: E[\Delta v / v]$, and $\beta^2 = 1 / \tau \:Var[\Delta v / v]$. -->
<!-- (See Hull [-@hull9thEdition] for details.) -->
<!-- Substituting $v(T)$ for $q$, $E[v(T)]\bigr|_{t = \hat{t}}$ for $E[q]$, and $Var[\ln(v(T))] \bigr|_{t = \hat{t}}$ for $\omega$, -->

Multiplying this through by the discount factor then gives the following closed form expression for real option value (ROV), evaluated at some time $t = \hat{t}$.

\begin{equation}
ROV = e^{-r\tau} E[max(x(T) - K, 0)]\bigr|_{t = \hat{t}} = e^{(m - r) \tau} x(\hat{t}) \Phi(\delta + s \sqrt{\tau}) - e^{-r tau} K \Phi(\delta)
\label{eq:rov}
\end{equation}

In the ROV literature, $m$ is usually set equal to $r$ due to an artifact inherited from the financial context called "risk neutral valuation". Farther below, I demonstrate that there is no theoretical or empirical premise for the assumption of risk neutral valuation in far-from-market ROV contexts; and thus no logical premise for setting $m = r$ here.

## Developing real options intuition

To build intuition, it is instructive to graph ROV (equation \ref{eq:rov}) against NPV (lefthand side of inequality \ref{eq:npvIneq2}). A generic example is given in Figure \ref{fig:rovIllust}. Note that ROV differs from NPV only over sufficiently large outlays $K$. Note also that, the smaller the uncertainty $s$, the smaller the range of $K$ over which there is a significant difference.

```{r Fig1, fig.show = "hold", fig.width = 3, fig.height=3, fig.align="center", fig.cap="\\label{fig:rovIllust}A hypothetical project's real option value and net present value plotted together over a range of outlays $K$. The black line marks 0, while the dotted red line marks the sunk cost $S$ beneath which the project is rejected. The multiple ROV plots correspond to different levels of uncertainty (s). The higher the uncertainty, the greater the difference between ROV and NPV.", echo = FALSE}


OVfun <- function(X0, K, tau, mm, s, r, output = "OV"){
  
  d2 <- (log(X0 / K) + (mm - s^2 / 2) * tau) / (s * sqrt(tau))
  d1 <- d2 + s * sqrt(tau)
  N1 <- pnorm(d1)
  N2 <- pnorm(d2)
  
  OV <- exp((mm - r) * tau) * X0 * N1 - exp(-r * tau) * K * N2

    if(output == "OV"){
    out <- OV
  }
  
  if(output == "N2"){
    out <- N2
  }
  
  if(output == "N1"){
    out <- N1
  }

  return(out)
}
#===========================================================================
mm <- 0.02
r <- mm
cv <- seq(1, 6, length.out = 3)
s <- round(cv * mm, 2)
X0 <- 10
tau <- 35
K <- seq(0.1, 3, length.out = 20) * X0
NPV <- X0 - exp(-r * tau) * K
ns <- length(cv)
list_df <- list()
for(i in 1:ns){
  this_s <- s[i]
  ROV <- OVfun(X0, K, tau, mm, this_s, r, output = "OV")
  df <- data.frame(K, cv = as.character(cv[i]), ROV, NPV)
  list_df[[i]] <- df
}

df_plot <- as.data.frame(do.call(rbind, list_df))
df_plot1 <- subset(df_plot, cv == cv[length(cv)])
df_plot1$cv <- NULL
df_plot1 <- df_plot1 %>% gather(type, Value, ROV:NPV)
df_plot2 <- subset(df_plot, cv != cv[length(cv)])

n <- length(unique(df_plot1$type))
bag_of_colors <- randomcoloR::distinctColorPalette(k = 2 * n)
color_vec <- sample(bag_of_colors, n)

gg <- ggplot()
gg <- gg + geom_line(data = df_plot2, aes(x = K, y = ROV,
                                         group = cv),
                     color = color_vec[2], lwd = 1)
gg <- gg + geom_line(data = df_plot1, aes(x = K, y = Value,
                                          group = type, color = type),
                     lwd = 1)
gg <- gg + scale_color_manual(values = color_vec)
gg <- gg + labs(y = "Value")
gg <- gg + geom_hline(yintercept = 2, color = "red", linetype = "dashed")
gg <- gg + geom_hline(yintercept = 0)
gg <- gg + theme_bw()
gg <- gg + theme(axis.text = element_blank(),
                 axis.title = element_text(size = axisTitle_size),
                 legend.title = element_blank(),
                 legend.position = "top",
                 legend.text = element_text(size = legendText_size))
gg


```

<!-- [ROV may not be new... old wine in new wine skins []... however, the ROV approach...provides a lot of decision-critical info in its formula...] -->
<!-- In addition to being a more realistic representation of...  the ROV approach   The terms $\Phi(d)$ and $\Phi(d + s \sqrt{\tau})$ provide additional information that can aid in decision making. -->

the underlying

in the money, out of the money, deep in the money, AR4D projects tend to be deep in the money but high risk. if not high risk then there would be no point in applying ROV, would be same as NPV

The term $\Phi(\delta)$ is very instructive. It is the probability of expiring in the money... that $x(T) > K$---in other words, the probability of project success. The term $\Phi(\delta + s \sqrt{\tau})$ is also instructive. It is equal to the partial derivative $\partial ROV / \partial x$, and thus provides insight into the sensitivity of real option value with respect to movements in the underlying project NPV.



## Real options with adandonment value \label{sec:abandVal}

It is easy and worthwhile to extend the ROV formula in equation \ref{eq:rov} to the slightly more general case where the project generates a positive benefit $B$ regardless of whether or not the project is successful. In the AR4D context, $B$ might correspond to the value of new or upgraded labs and testing facilities, and/or improved human capital through training. To accommodate such cases, the ROV formula can be generalized as follows.

\begin{equation}
ROV = e^{-r T} E[\max(x(T) - K, B)] \:\:;\:\:\: B \geq 0
\label{eq:rovBraw}
\end{equation}

Such that the decision criterion becomes

\begin{equation}
e^{-r T} E[\max(x(T) - K, B)] < S \:\: \rightarrow \:\: \text{reject project}
\label{eq:rovBrawCond}
\end{equation}

But note that, by the formula for $\max(a, b)$, i.e.,

\begin{equation}
\max(a, b) = \frac{1}{2} (a + b - \left| a - b \right|)
\end{equation}

Equation \ref{eq:rovBraw} can be rewritten

\begin{equation}
ROV = e^{-r T} (E[\max(x(T) - K + B, 0)] + B)
\end{equation}

And hence, by equation \ref{eq:part1},

\begin{equation}
\begin{split}
ROV &= e^{(m - r) \tau} x(\hat{t}) \Phi(\delta + s \sqrt{\tau}) - e^{-r \tau} (K - B) \Phi(\delta) + B \\
&= e^{(m - r) \tau} x(\hat{t}) \Phi(\delta + s \sqrt{\tau}) - e^{-r \tau} K \Phi(\delta) +  e^{-r \tau} B( 1 + \Phi(\delta))
\end{split}
\label{eq:rovB}
\end{equation}

Where

\begin{equation}
\delta = \frac{\ln \left(\frac{x(\hat{t})}{K - B} \right) + \left(m - \frac{s^2}{2} \right) \tau}{s \sqrt{\tau}}
\end{equation}

And where it is easy to see that this reduces to equation \ref{eq:rov} when $B = 0$.

## Quantifying project NPV volatility \label{sec:volEst}

AR4D projects are characterized by high, potentially disruptive, expected net benefits, but also by extreme uncertainty surrounding these expected net benefits. Expected project impacts are vulnerable to a host of both research and non-research related factors (to be discussed in more detail farther below), such that they can easily double or halve over the life of the project. Project appraisals that do not price in, or at least provide some treatment of, this vast uncertainty are thus of limited use to prospective donors.
<!-- given new additions to or subtractions from the list of countries targeted for release of the new technology. -->

One of the appealing advantages of the ROV approach, then, is precisely that it prices in project volatility. However, there remains the question of how to quantify vaolatility. In the financial context, a direct calculation of volatility is possible based on easily accessible historical price time series. In the real options context, direct calculation is generally not possible, since analogous historical project NPV time series data usually do not exist.

In practice, applied ROV studies generally make an educated guess at the volatility parameter, and then conduct sensitivity analysis around the guess (as, for example, in Majd and Pindyck [-@majd1987time] or Kemna [-@kemna1993case]). While this approach is, in some sense, pragmatic, it is also problematic, because it is difficult to build intuition about what constitutes a plausible volatility parameter value for a given project. Sensitivity analysis is one way of building such intuition, but it is a blunt, laborious instrument; and the intuition gained as a result generally does not transfer to other projects. In an alternative, novel approach, Pennings and Lint [-@pennings1997option] directly calculate the volatility parameter based on a painstakingly assembled project NPV time series. While clearly more rigorous than the former approach, this is less expedient and probably not possible under most AR4D time, resource, and data constraints.

To find an intermediate point on the rigor-pragmatism spectrum, consider: While research managers (and/or stakeholders and/or the foresight economists conducting ex-ante impact assessments) generally do not have an intuitive grasp of the project NPV volatility parameter itself, they do generally have some idea of the uncertainty surrounding project NPV estimates, such that they can be asked to quantify minimum and maximum bounds on project NPV, in addition to the NPV estimation itself. For example, they can say that NPV is $x(\hat{t})$, but that it might be as much as $100 \%$ greater than $x(\hat{t})$ or as little as $50\%$ lower than $x(\hat{t})$. [Or they may give a magnitude, which is then easily transformed into percentage terms.]

These upper and lower percentage errors may be interpreted as the bounds on the $95\%$ confidence interval about the mean of the log return $\mu$

Interpreting these upper and lower percentage errors as the bounds on a $95\%$ confidence interval, both the NPV volatility and the mean NPV growth rate can usually be deduced from this information. If $x(t)$ is a gBm, for example, then the upper and lower percentage errors, $\overline{y}$, $\underline{y}$, may be expressed as follows.

\begin{equation}
\begin{split}
\overline{y} &= \mu + z \sigma \\
\underline{y} &= \mu - z \sigma
\end{split}
\end{equation}

Where $z = 1.96$ is the standard score corresponding to the $95\%$ confidence interval of a normally distributed random variable, and

\begin{equation}
\begin{split}
\mu &= \left(m - \frac{s^2}{2} \right) \tau \\
&= E \left[ \ln \left( \frac{x(T)}{x(\hat{t})} \right) \right] \approx E \left[ \frac{x(T) - x(\hat{t})}{x(\hat{t})} \right] \\
\sigma &= s^2 \tau
\end{split}
\end{equation}

This system of equations may then be solved for the project volatility parameter $s$ and mean growth rate $m$ in terms of the percentage errors, the project time horizon $T$, and the standard score $z$, as follows.

\begin{equation}
\begin{split}
s &= \frac{1}{2 \sqrt{\tau} z} (\overline{y} - \underline{y}) \\
m &= \frac{1}{2 \tau} \left( \overline{y} + \underline{y} + \frac{(\overline{y} - \underline{y} )^2}{4 z^2} \right)
\end{split}
\end{equation}
<!-- yMax <- 1.1 -->
<!-- yMin <- -0.6 -->
<!-- # Define confidence interval of interest, usually 95% -->
<!-- zBound <- 2#1.96 -->
<!-- # Back out s and m -->
<!-- s <- (yMax - yMin) / (2 * zBound * sqrt(tauN)) -->
<!-- mu <- (yMax + yMin) / 2 -->
<!-- m <- (mu / tauN + s^2 / 2) -->
<!-- m <- 1 / (2 * tauN) * (yMax + yMin + (yMax - yMin)^2 / (4 * zBound^2)) -->

The coefficient of variation $s \over m \sqrt{\tau}$ is a handy, normalized measure of project NPV volatility over time. One should not be surprised to see coefficients of variation of 1 or greater in the AR4D context. Since AR4D real options are usually deep in the money, coefficients of variation less than 1 may indicate that there is little point in calculating ROV, as it will probably not differ significantly from the default approach. Note also that, in the process of deducing $s$ and $m$, the expected project NPV log return $\mu$ is calculated, and that the expected project NPV at future times may be calculated by equation \ref{eq:ExT}. These calculations may serve as further checks on the plausibility of chosen parameter values, project NPV estimates, and project NPV upper and lower bounds.

The standard score can of course be adjusted to match the practitioner's idea of a suitable confidence interval defined by the bounds $\overline{y}$ and $\underline{y}$. Note that the system of equations above can still be solved for $m$ and $s$ in the absence of an estimate for one of the bounds $\overline{y}$ or $\underline{y}$, if an estimate for the expected change in NPV $\mu$ can be obtained.

## The $n$-fold real option value of multi-stage AR4D projects \label{sec:resStages}

<!-- [@mitchell1988managing] -->
As mentioned above, AR4D projects typically unfold in a series of stages. While details vary depending on crop, technology, research institution, and project aims and constraints, project stages may be defined very broadly as follows.

Pre-project: A discovery or "blue skies research" stage, during which new technologies are "discovered" through a careful exploration and extension of existing research---in which serendipity plays a key role---in conjunction with a careful assessment and prioritization of research demand, target populations and ecologies, and the relevant socioeconomic enabling environments (particularly seed systems, government policies, institutions, and markets).

1) A basic replication or scaling up stage, whereby a technology proof-of-concept generated in the discovery stage---typically in the form of a genotype with a novel trait, together with a set of recommended farm management practices---is reproduced in greenhouse and/or confined field trials.

2) A multi-location, multi-season testing stage, whereby successful specimens generated in the preceding stage are taken for further scaling up and testing under distinct agronomic conditions over multiple cropping seasons. [[Marc - mention marker assisted]]

In the case of transgenic research, this is typically followed by:

3) A regulatory dossier stage, wherein detailed agronomic, environmental, and toxicological data, mostly generated during the preceding stages, is compiled into dossier(s) for submission to the National Competent Authorities (NCAs) in the target countries.

4) A deregulation stage, during which the regulatory dossier(s) generated during the previous stage is/are submitted to the NCA(s), which may request further clarification and testing of certain aspects of the proposed technology.

In today's increasingly holistic view of AR4D [@klerkx2010adaptive], it may also make sense to enumerate:

Post-project: A release and uptake stage, during which the new technology is made available for distribution to target populations, typically some 2-3 decades after the start of stage 2. This stage depends critically on the quality of the socioeconomic enabling environment in the target countries, in conjunction with engagement from researchers to ensure correct application of the new technology at farm level.

Traditionally, stages 2-5 constitute a single AR4D project. Donors generally do not engage in the discovery phase. Stage 6 involves some additional greenhouse and CFT work to adapt the new technology to the specific country of release. Details may vary widely depending on whether the new technology is released as a public good or under private contract.

<!-- through separate program level arrangements, since investments there are cross-cutting, i.e. they apply across multiple prospective technologies Investments in stage 6 are also cross-cutting to the extent that they involve support of extension services, seed systems, and other value chain mechanisms. -->

The duration of project stages varies depending on technology, crop, and research institution; but, as a general rule, non-transgenic AR4D projects (stages 2-3) last 15-25 years, while transgenic projects (stages 2-5) tend to last 10-15 years. (Increasingly, the longer time horizons of non-transgenic research may be shortened through marker assisted breeding.)
<!-- Such long time horizons mean vast uncertainty surrounding any estimate of project NPV---on the order of, say, a coefficient of variation of at least $2$. Methods of project valuation that do not take account of such extreme uncertainty are effectively meaningless. As shown above, the ROV approach accommodates uncertainty through the parameter $s$. -->
<!-- The aims and scope of AR4D projects are typically very ambitious, with disruptive implications across multiple populations if successful. However, they are also extremely risky, with time horizons spanning multiple election cycles, usually in politically volatile developing countries. This means that percentage changes in project NPV  -->
<!--  the costs of the so-called ‘discovery phase’ are excluded from this study.  -->
<!-- The cost assessment begins at the clearly defined milestone where the proof-of-concept is  -->
<!-- completed  and  the  technology  is  ready  for  product  development.  In  the  case  of  LBr  -->
<!-- potato, that means that the gene sequence, including its regulatory elements, the method  -->
<!-- of  transformation,  the  expected  level  of  expression  in  the  target  organ,  the  stage  of  -->
<!-- development of the plant, and the trait assessment, are all known. Thus, none of the costs  -->
<!-- associated with research and development of the gene constructs or of testing them in  -->
<!-- transgenic  events  are  included  (cloning  and  testing  different  R  genes,  testing  the  -->
<!-- durability  of  LB  resistance,  evaluating  different  strategies  for  deployment,  socio- -->
<!-- economic  targeting  studies,  communicating  the  results  to  stakeholders,  and  building  -->
<!-- biotechnology  and  biosafety  facilities).  Also  excluded  are  the  costs  of  building  the  -->
<!-- capacity  of  partners,  scientific  publications,  and  participation  in  scientific  conferences  -->
<!-- apart from any such activities strictly needed for the LBr product development. The costs  -->
<!-- of  obtaining  freedom-to-operate  and/or  intellectual  property  rights  over  the  relevant  -->
<!-- technology are also excluded. It is assumed that these issues and costs have been dealt  -->
<!-- with  at  the  previous  stage  of  the  proof-of-concept  and  has  defined  which  technology  -->
<!-- element will be eventually used for product development. -->

In the default approach to project funding decisions, the decision criterion for multi-stage projects may be formalized as follows.

\begin{equation}
e^{- r T_n} E[x(T)] \bigr|_{t = 0} - \Sigma_{i = 1}^{n - 1} e^{-r T_i} K_i < K_n \:\: \rightarrow \:\: \text{reject project}
\label{eq:npvIneqN1}
\end{equation}

Or, if $x(t)$ follows a gBm,

\begin{equation}
e^{(m - r) T_n} x(0) - \Sigma_{i = 1}^{n - 1} e^{-r T_i} K_i < K_n \:\: \rightarrow \:\: \text{reject project}
\label{eq:npvIneqN2}
\end{equation}

This is a straightforward generalization of expression \ref{eq:npvIneqN2} to $n$-stage projects.

To formalize the $n$-fold real option value of AR4D projects, it is helpful to first do some relabeling. Let the real option value of a single stage project (equation \ref{eq:rovBraw}) evaluated at some time $\hat{t}$ henceforth be relabeled $f_1(\hat{t})$, with time horizon $T_1$, exercise cost $K_0$, and abandonment value $B_0$. And let $f_0(t) = x(t)$. Then equation \ref{eq:rovBraw} can be written

\begin{equation}
f_1(\hat{t}) = e^{-r \tau_1} E[\max(f_0(T_1) - K_0, B_0)] \bigr|_{t = \hat{t}}
\end{equation}

(Where $\tau_i = T_i - \hat{t}$.)

As explained above, this is the value of the option, but not the obligation, to disburse the funds required to cover final launch and scaling up costs $K_0$ once the research project is finished. Now consider the real option value of a 2-stage project $f_2(\hat{t})$.

\begin{equation}
f_2(\hat{t}) = e^{- r \tau_2} E[\max(f_1(T_2) - K_1, B_1)] \bigr|_{t = \hat{t}}
\end{equation}

This is the value of the option, but not the obligation, to disburse the funds $K_2$ required to implement the second stage of the 2-stage project, once the first stage is finished. Note that $K_1$ corresponds to the stage $n$ sunk cost $S$ in equation \ref{eq:rovBrawCond}. The time horizon $T_2$ refers to the duration of stage 1, and $B_1$ is the abandonment value, if any, generated during stage 1.

Likewise, for a 3-stage project,

\begin{equation}
f_3(\hat{t}) = e^{- r \tau_3} E[\max(f_2(T_3) - K_2, B_2)] \bigr|_{t = \hat{t}}
\end{equation}

Where $T_3$ is the duration of the first stage, $B_3$ is any abandonment value associated with this stage, and $K_3$ the cost of implementing the second stage, of the 3-stage project.

And so on, for any $n$-stage project, the $n$-fold option value $f_n(\hat{t})$ may be defined

\begin{equation}
f_n(\hat{t}) = e^{- r \tau_n} E[\max(f_{n - 1}(T_n) - K_{n - 1}, B_{n - 1})] \bigr|_{t = \hat{t}} \:\:;\:\:\: B_{n - 1} \geq 0
\label{eq:rovNraw}
\end{equation}

In other words, $n$-fold option value is the present value of the option to continue with stage 2 of the project, the value of which is itself the present value of the option to continue with stage 3 of the project, and so forth, up until the present value of the option to continue with stage $n$ of the project, which is itself the present value of the option to implement the new technology, which is itself the project NPV $x(t)$.

The 1-stage ROV funding decision criterion (inequality \ref{eq:rovBrawCond}) may thus be generalized to $n$-stage projects as follows.

\begin{equation}
e^{-rT_n} E[max(f_{n - 1}(T_n) - K_{n - 1}, B_{n - 1})] < K_n \:\: \rightarrow \:\: \text{reject project}
\end{equation}

Or, more compactly,

\begin{equation}
f_n < K_n \:\: \rightarrow \:\: \text{reject project}
\label{eq:rovNcond}
\end{equation}

The exercise cost of a given stage is the cost of implementing the subsequent stage.

Farther below, I demonstrate that, if $x(t)$ is lognormally distributed, then so is $f_n(t)$. Equation \ref{eq:part1} may therefore be invoked to establish a closed form expression for $f_n(\hat{t})$, if $x(t)$ is lognormally distributed. Before getting to this main result, however, it is first necessary to defend the assumption of lognormal $x(t)$, and to extract far-from-market real options from the thicket of "risk neutral valuation".

# Modeling the evolution of R&D project NPV

When evaluating R&D projects as real options, it becomes necessary to think carefully about how project NPV changes over time. This, in turn, requires careful consideration of the causes behind changes in project value. A reasonable starting point in this consideration is the observation that changes in R&D project value seem to be of two types: research related and non-research related.

Research related changes in project value generally occur at discrete test points, when new information regarding the effectiveness of the new technology becomes available. Non-research related changes in project value occur as a result of changes in the political, socio-economic, and institutional enabling environments where the new technology is to be released. Such changes may include, for example, elections, abrupt changes in government policies, commodity price swings, changes in seed systems and other value chain mechanisms, changes in the security environment, the ebb and flow of public and private sector partnerships to enhance impact, and so forth.

Research related changes occur perhaps 1-4 times per year, while non-research related changes occur with greater frequency, perhaps 1-4 times per quarter. Overall, then, it is reasonable to expect changes in AR4D project value to occur every quarter as in the left panel of Figure \ref{fig:NPVevol}.

```{r Fig2, fig.show = "hold", fig.width = 6, fig.height=3, fig.align="center", fig.cap="\\label{fig:NPVevol}(Left) NPV changing every time step. (Right) NPV changing only in certain time steps.", echo = FALSE}

# library(tidyverse)
# library(patchwork)
#----------------------------------------------------------------------------
gbmFun <- function(tau = 40, m = 0.001, s = 0.003, x0 = 1, randVec = NULL) {
  if(is.null(randVec)){randVec <- rnorm(tau)}
  epsilon <- randVec
  lx <- c(); lx[1] <- log(x0)
  drift <- (m - s * s / 2)
  for(t in 2:tau){
      dBt <-  s * epsilon[t]
      lx[t] <- lx[t - 1] + drift + dBt
  }
  x <- exp(lx)
  return(x)
}
# m <- 0.001
# s <- 0.003
# tau <- 400
# x0 <- 1
# x <- gbmFun(tau, m, s, x0, randVec = NULL)
# df_plot <- data.frame(t = 1:tau, x)
# gg <- ggplot(df_plot, aes(x = t, y = x))
# gg <- gg + geom_line()
# acf(diff(log(x)))
# hist(diff(log(x)))
# shapiro.test(diff(log(x)))
#---------------------------------------------------------------------------
# Algorithm 6.2 in Tankov 2003 Financial modeling with jump processes
compoundPoisFun <- function(tau, lambda, m_y = 0, s_y = 1, maxiter = 500){
  N <- rpois(1, lambda * tau)
  U <- runif(N) * tau
  U <- round(U)
  n_iter <- 0; flag <- 1
  while(flag == 1){
    n_iter <- n_iter + 1
      U <- runif(N) * tau
      U <- round(U)
    if(sum(duplicated(U)) > 0 & n_iter <= maxiter){
      flag <- 1
    }else{
      flag <- 0
      if(n_iter == maxiter){
        print("Reached maxiter without generating duplicate-free event times vec. Dropping duplicates.")
        U <- U[-which(duplicated(U))]
      }
    }
  }
  Tt <- U[order(U)]
  J <- exp(rnorm(N, m_y, s_y)) - 1
  #---
  cumJ <- cumsum(J)
  # For explicit modeling of Yt per time step
  Yt <- rep(0, tau)
  Yt[Tt] <- cumJ
  #---
  Tt <- c(0, Tt)
  J <- c(0, J)
  if(Tt[length(Tt)] != tau){
    Tt <- c(Tt, tau)
    J <- c(J, 0)
  }
  cumJ <- cumsum(J)
  df_step <- data.frame(Tt, cumJ)
  #--------------------------
  nEvents <- length(Tt)
  cumYt <- c()
  for(i in 1:(nEvents - 1)){
    tStart <- Tt[i]
    tFin <- Tt[i + 1] - 1
    cumYt[tStart:tFin] <- cumJ[i]
  }
  cumYt[tau] <- cumYt[tFin]
  df_Yt <- data.frame(t = 1:tau, Yt, cumYt)
  
  list_out <- list(df_step, df_Yt)
  return(list_out)

}
#----------------------------------------------------------------------------
FractDim<-function(Data,graphon=FALSE) {
  X=Data;N=length(X);
  jstart=10;jend=floor(10*(log10(N)-1));
  kvec=c(1:4,floor(2^(c(jstart:jend)/4)));
  indkend=length(kvec);
  k=c()
  AvgLmk=c()
  err=c()
  for(indk in 1:indkend)
  {
    k=kvec[indk]
    Xend=c()
    Xsum=c()
    Lmk=c()
    for(m in 1:k)
    {
      Xend=floor((N-m)/k)
      Xsum=sum(abs(X[m+c(1:Xend)*k]-c(0, X[m+c(1:(Xend-1))*k])))
      Lmk[m]=1/k*1/k*(N-1)/Xend*Xsum
    }
    AvgLmk[indk]=mean(Lmk)
    #  err[indk]=sd(log(Lmk))
  }
  x<-log(kvec)
  y<-log(AvgLmk)
  q<-lm(y~x)
  slope<-q$coefficients[2]
  yintcept<-q$coefficients[1]
  yfit<-x*slope+yintcept
  FrDim <- -slope
  avgRes <- mean(abs(q$residuals))
  if(graphon==TRUE)
  {
    plot(x,y,main="If linear then fractal, w/Fr. Dim = (-)slope",xlab="Ln(k)",ylab="Ln(length of curve with interval k)")
    z<-line(x,yfit);abline(coef(z),col='blue');z<-NULL
    #z<-line(x,y);abline(coef(z),col='blue');z<-NULL
  }
  #z<-line(x,y);qq=coef(z)
  #yintcept=qq[1]
  #FrDim=-qq[2]
  return(c(FrDim, avgRes, yintcept))
}
#---------------------------------------------------------------------------
m <- 0.005
s <- m * seq(5.5, 50, length.out = 3)
tau <- 48
x0 <- 1
#rn <- round(runif(1) * 1000)
#rn <- 905
#rn <- 517
# rn <- 340
# set.seed(rn)
# tau <- 12 * 4
lambda <- 0.1


#----------------------------------------------------------------------------
#randVec <- coloredNoise(N = tau, alpha = 1, scaleIt = T)
#acf(randVec)
list_df <- list()
list_spec <- list()
facet_labels <- c()
for(i in 1:length(s)){
  x <- gbmFun(tau, m, s[i], x0, randVec = NULL)
  df_x <- data.frame(t = 1:tau, x, s = as.character(s[i]))
  list_df[[i]] <- df_x
  o <- spectrum(x)
  df_gbmSpec <- data.frame(lfreq = log(o$freq), lpwr = log(o$spec))
  df_gbmSpec <- df_gbmSpec[-1, ]
  df_gbmSpec$s <- as.character(s[i])
  list_spec[[i]] <- df_gbmSpec
  
  mod <- lm(lpwr ~ lfreq, df_gbmSpec)
  # summary(mod)
  alpha <- round(as.numeric(coefficients(mod)[2]), 2)
  # yint <- as.numeric(coefficients(mod)[1])
  # df_out <- as.data.frame(broom::glance(mod))
  # adjR2 <- round(df_out$adj.r.squared, 2)
  # N <- df.residual(mod)
  this_facet_label <- paste0("Slope = ", alpha, " fd = ", round(FractDim(x)[1], 2))
  facet_labels[i] <- this_facet_label
}
#---
df_plot <- as.data.frame(do.call(rbind, list_df))
colnames(df_plot)[1:2] <- c("Time", "Project NPV")
df_plot <- subset(df_plot, s == s[2])
gg <- ggplot(df_plot, aes(x = Time, y = `Project NPV`))
gg <- gg + geom_line()
#gg <- gg + facet_wrap(~s, scales = "free_y", ncol = 1)
gg <- gg + theme_bw()
gg <- gg + theme(axis.text = element_blank(),
                 axis.title = element_text(size = axisTitle_size))
gg_gbm <- gg
#---
df_plot <- as.data.frame(do.call(rbind, list_spec))
colnames(df_plot)[1:2] <- c("Logged frequency", "Logged power spectral density")
df_plot <- subset(df_plot, s == s[2])
#names(facet_labels) <- s
gg <- ggplot(df_plot, aes(x = `Logged frequency`, y = `Logged power spectral density`))
gg <- gg + geom_smooth(method = lm, se = F)
gg <- gg + geom_line()
# gg <- gg + facet_wrap(~s, ncol = 1,
#                       labeller = labeller(s = facet_labels))
gg <- gg + theme_bw()
gg <- gg + theme(axis.text = element_blank(),
                 axis.title = element_text(size = axisTitle_size))
gg_gbmSpec <- gg
#----------------------------------------------------------------------------
list_df <- list()
list_spec <- list()
facet_labels <- c()
for(i in 1:length(s)){
  #s_y = 0.3
  list_out <- compoundPoisFun(tau, lambda, m_y = m, s_y = s[i], maxiter = 500)
  df_x <- list_out[[1]]
  df_x$s <- as.character(s[i])
  list_df[[i]] <- df_x
  
  df_Yt <- list_out[[2]]
  o <- spectrum(df_Yt$cumYt)
  df_spec <- data.frame(lfreq = log(o$freq), lpwr = log(o$spec))
  df_spec <- df_spec[-1, ]
  df_spec$s <- as.character(s[i])
  list_spec[[i]] <- df_spec
  
  mod <- lm(lpwr ~ lfreq, df_spec)
  # summary(mod)
  alpha <- round(as.numeric(coefficients(mod)[2]), 2)
  # yint <- as.numeric(coefficients(mod)[1])
  # df_out <- as.data.frame(broom::glance(mod))
  # adjR2 <- round(df_out$adj.r.squared, 2)
  # N <- df.residual(mod)
  this_facet_label <- paste0("Slope = ", alpha, " fd = ", round(FractDim(df_Yt$cumYt)[1], 2))
  facet_labels[i] <- this_facet_label

}
#---
df_plot <- as.data.frame(do.call(rbind, list_df))
colnames(df_plot)[1:2] <- c("Time", "Project NPV")
df_plot <- subset(df_plot, s == s[2])
gg <- ggplot(df_plot, aes(Time, `Project NPV`))
#gg <- ggplot(df_cpp, aes(t, cumYt))
gg <- gg + geom_step()
#gg <- gg + facet_wrap(~s, ncol = 1)
gg <- gg + theme_bw()
gg <- gg + theme(axis.text = element_blank(),
                 axis.title = element_blank())
gg_cpp <- gg
#---
df_plot <- as.data.frame(do.call(rbind, list_spec))
colnames(df_plot)[1:2] <- c("Logged frequency", "Logged power spectral density")
df_plot <- subset(df_plot, s == s[2])
#names(facet_labels) <- s
gg <- ggplot(df_plot, aes(x = `Logged frequency`, y = `Logged power spectral density`))
gg <- gg + geom_smooth(method = lm, se = F)
gg <- gg + geom_line()
# gg <- gg + facet_wrap(~s, ncol = 1,
#                       labeller = labeller(s = facet_labels))
gg <- gg + theme_bw()
gg <- gg + theme(axis.text = element_blank(),
                 axis.title = element_blank())
gg_cppSpec <- gg
#===========================================================================

gg_gbm + gg_cpp + plot_layout(nrow = 1)


```

This conclusion implies a subtle but fundamental decision to model changes in R&D project value _when they occur_, as opposed to when project managers learn of their occurrence. In the corporate R&D context, Pennings and Lint [-@pennings1997option] take the opposite approach, registering changes in R&D project value only when management becomes aware of the changes through internal analysis and reporting protocols. In other words, they model changes in project value as "information dam-breaks", whereby new information affecting project value accumulates for a time until it is suddenly released in a report to management, which only then updates the project's value. Such a model of project NPV evolution is depicted in the right panel of Fig. \ref{fig:NPVevol}.

The information dam-break approach may be appropriate for corporate R&D, where the value of company assets and activities is ultimately defined by the stock market, and hence (in theory) by publicly available information. (Indeed, information dam-breaks are the premise of frontrunning, and hence the bread and butter of many an investment bank.) In the far-from-market R&D context, however, no such nuance comes into play.

Whether the evolution of project NPV (or of any stochastic process) resembles the time series on the left or right of Figure \ref{fig:NPVevol} is, to a certain degree, a mere matter of the choice of time step. That is to say, the time series on the left can be transformed into to something resembling the time series on the right merely by plotting over a smaller time step. Likewise, the model on the right can be transformed into the model on the left by plotting over sufficiently large time step. The model on the right becomes unavoidable when said sufficiently large time step results in an unacceptably small number of time steps. In the AR4D context, this is generally not a problem. Projects typically last 9-25 years, such that, given a quarterly time step, the project NPV time series typically contains 36-100 steps.

The time series on the left of Figure \ref{fig:NPVevol} is an example of geometric Brownian motion (gBm), while the time series on the right is a compound Poisson process (algorithm 6.2 in Cont and Tankov [-@tankov2003financial]). The compound Poisson process (cPp) assumes that changes in value occur at random time intervals. This is fine in the financial context, but may be problematic in real options contexts, where the information affecting project NPV is released through reports to management---events that are usually, at least in the AR4D context, non-random.

## In defense of the gBm model of project NPV

<!-- [Rejection of the gBm model in ROV contexts is partly inherited from the financial context, where gBm is generally viewed as a naive model of price movements. fat tails--debunked by Tankov, jumps--as discussed above, an artifact of the dam-break model, not a concern in ROV contexts, plus jumps can be closely approximated by gBm. en fin, gBm is much more versatile than it is generally given credit for.] -->
Like Pennings and Lint, many real options authors reject the gBm model, preferring instead to select a model which they perceive to more accurately approximate the real time evolution of project NPV. But this comes at a substantial loss of expedience, as the closed form expression in equation \ref{eq:rov} must be replaced by considerably less tractable, less transparent, and less instructive expressions, up to and including numerical methods. Trigeorgis once characterized numerical methods as the unavoidable "bitter pill" that every serious ROV practitioner must come to grips with [-@trigeorgis1993real].

It seems safe to say, in hindsight, that energetic proponents of ROV like Trigeorgis have overestimated the appetite for bitterness outside of academic audiences. Adoption of ROV thinking by real world decision makers has remained low [@horn2015use; @triantis2005realizing; @driouchi2012real]. ROV critics and proponents alike attribute the tepid reception to the formal complexity of ROV, much of which can be traced back to the use of numerical methods [@triantis2005realizing]. From the perspective of research donors and managers, numerical methods are effectively black boxes. Even ROV experts find themselves bamboozled at times. The much cited numerical exercise by Majd and Pindyck [-@majd1987time], for example, contains an elementary error, which was only discovered some ten years after publication [@milne2000time].
<!-- [---enough so, at any rate, to warrant some revisiting of the matter:] -->
<!-- The case for complex ROV approaches weakens further when considering what is gained in return for sacrificing expedience. Pennings and Lint find only a $2\%$ difference between the the output of their cPp numerical model and that of the gBm closed form model [-@pennings1997option]. To what extent is an increase in modeling realism even meaningful in a context where direct measurement of the reality one aspires to approximate---i.e., the evolution of project NPV---is highly problematic? Is the costly increase in realism even relevant to the aims of the modeling exercise? Is there an alternative, less costly way of achieving the same ends? -->
<!-- 33.1 - 32.3 -->

In any methodological decision, there is usually a tradeoff between realism and expedience. The job of the modeler is thus not merely to maximize realism, but to strike the optimal balance between realism and expedience under the particular time and resource constraints of the modeling exercise at hand, and in a way that is clearly relevant to the particular objectives of the exercise. This is especially true in real world decision making contexts, where constraints are considerably more severe than in academic contexts. [And it must be kept in mind that donor patience for long, technical explanations, must be counted among the resources in short supply.]

Before making costly methodological decisions in the name of realism, then, one must carefully consider 1) to what extent an "increase in realism" is even meaningful, i.e., to what extent it is possible to define and measure the reality one aspires to model; 2) whether the proposed increase in realism is actually relevant to the objectives of the modeling exercise, or whether it is just realism for realism's sake; and 3) whether there is a simpler, less costly way to achieve the same increment in realism, or the same modeling objectives which the increment in realism is supposed to serve. [One must also be careful of merely replacing one unrealistic artifact with another.] 

In the financial context, the reality one aspires to model---i.e. some stochastic financial process, usually a price series---is well defined in the form of historical time series that can be easily downloaded, measured, analyzed, etc. In far-from-market R&D contexts, by contrast, analogous historical time series of project NPV generally do not exist. The reality one aspires to model must be perceived indirectly, based primarily on assumptions and logic, as I have just done above. So, in far-from-market real options contexts, it is not even entirely clear what one gets in return for sacrificing expedience.

Then there is the question of the relevance of the increase in realism. In the financial context, the relevance of a realistic model of the time evolution of specific stochastic processes is clear. That is, it can make the difference between profit and loss. In far-from-market real options contexts, on the other hand, there is no real need for a good predictor of exactly when or in what order specific changes in project NPV occur. The overarching purpose of modeling project NPV is to calculate its real option value; and for this, the relevant question is rather one of the size distribution of changes. The cPp model might seem more realistic than the gBm model when trying to approximate a time series in which there are a few big changes interspersed by long periods of no change. However, the gBm model can approximate such a time series arbitrarily closely, so long as it is acceptable to substitute "periods of no change" with "periods of negligibly small changes". The periodograms given in Figure \ref{fig:NPVevol2}, which correspond to the time series in the previous Figure, indicate that the size distribution of changes of the two time series is similar, despite their differences in the time domain.

```{r Fig2, fig.show = "hold", fig.width = 6, fig.height=3, fig.align="center", fig.cap="\\label{fig:NPVevol2}(Left) Periodogram of the time series in the left panel of previous figure. (Right) Periodogram of the time series in the right panel of previous figure.", echo = FALSE}

gg_gbmSpec + gg_cppSpec + plot_layout(nrow = 1)

```

This then answers the final question of whether or not a comparable increase in realism, or the objective which the desired increase in realism is supposed to serve, may be achieved at lower cost. In the case of cPp, the graphic above  suggests that yes, the same realism may be achieved at no cost of expedience by using the gBm model. This closeness is reflected, for example, in the results of Pennings and Lint, who find only a $2\%$ difference between ROV as calculated by their cPp-based numerical model and ROV as calculated by the gBm-based closed form model [-@pennings1997option].
<!-- Given all the research and non-research factors affecting project NPV, it is highly unlikely that there is not at least a small, if negligible, perturbation in NPV in every time step, in which case the gBm model is the more realistic choice (when modeling project NPV evolution as it happens, as opposed to the information dam-break approach). -->
<!-- The question is not when changes in NPV occur, but rather how often do substantial changes to NPV occur? In more technical terms, what is the size distribution of changes? In order to answer this question, it is more instructive to look at the signal's periodogram, not its evolution in the time domain. This can be examined by looking at a periodogram. Model 1 can effectively approximate model 2 to an arbitrary degree of precision by tuning the uncertainty parameter $s$ (Figure \ref)..... A few substantial changes followed by relatively long periods of little change. [The Poisson jump model represents a process in which there are a few substantial changes interspersed among periods of no changes at all. While the gbm model cannot replicate periods of no change exactly, it can approximate such periods arbitrarily closely through adjustments to the volatility parameter.... And recall that it is highly unlikely that there are no changes in project value in any given time step, but rather that there may be long periods of very small changes punctuated by brief periods of large changes. This is perhaps best illustrated by looking at periodograms (Figure ...).] -->
<!-- Geometric Brownian motion is a much more versatile model than portrayed in the literature.... "bitter pill" [trigergis]. criticism fat tails etc. starting with Mandelbrot []. but this has led to misconception... [Tankov]. The fact is that gbm remains a highly versatile model capable of representing a wide variety of stochastic processes by adjustments to the volatility parameters. The key question that Pennings and Lint address with their jump model may be formulated as follows: what is the size distribution of changes in project value?  [P&L Poisson jump model output differed from the lognormal assumption output by just x% [P&L].] -->
<!-- [For the purposes of evaluating real option value, the question is not so much when exactly the changes occur, but rather their size distribution. In other words not the time domain but the frequency domain that is important.] -->
<!-- as compared to the default NPV approach -->
<!-- ## Low adoption of real options thinking due to complexity -->
<!-- Despite a flood of academic interest in ROV following Myers' initial insight, adoption of the real options approach by real world decision makers remains low [@horn2015use; @triantis2005realizing; @driouchi2012real]. ROV critics and proponents alike attribute this tepid reception to the formal complexity of evaluating and explaining ROV as compared to the default NPV approach [@triantis2005realizing]. -->
<!-- Much of this complexity can be traced back to two sources. Firstly, there is the frequent and puzzling assumption of risk neutral valuation in far-from-market real options contexts, just mentioned above. Secondly, many authors reject the key assumption that project NPV evolution may be modeled as a gBm, preferring instead to evaluate equation \ref{eq:rovRaw} by numerical methods that are considerably less tractable, transparent, and instructive than equation \ref{eq:rov}. Trigeorgis, for example, calls numerical methods a "bitter pill" that every serious ROV practitioner must come to grips with [-@trigeorgis1993real]. -->

<!-- Before deriving the $n$-fold ROV model, it is first necessary to redress these two sources of confusion in detail. It seems safe to say, in hindsight, that energetic ROV proponents like Trigeorgis may have overestimated the appetite for bitterness outside of academic audiences. After this introductory section, I preface the derivation of the $n$-fold ROV model with a defense of the gBm model of project NPV, followed by a repudiation of risk neutral valuation in far-from-market real options contexts. -->
<!--   present arguments in defense of gBm as a model of far-from-market project NPV. In particular, I note that the a as a much more versatile model than it is given credit for. -->
<!-- Secondly, there is confusion regarding the interpretation, in real options contexts, of the financial artifact known as "risk-neutral valuation". -->
<!-- (see, for example, Hayes and Garvin [-@hayes1982managing], McGrath and MacMillan [-@mcgrath2000assessing], Doctor, Newton, and Pearson [-@doctor2001managing], and Newton, Paxson, and Widdicks [-@newton2004real]), -->
<!-- [However, the question of where ... must be assessed on a case by case basis. is a matter of preference. In academic contexts, there is a premium on rigor and realism. Most real options settings, on the other hand, time, resources, patience, and attention-span are in relatively short supply ... there is a premium on transparency, expediency. Pennings and Lint use equation \ref{eq:rov} and find that the numerical method output differs by just x%. That is a lot of extra work for a negligible difference. Whether or not changes occur in every time step is a matter of judicious choice of time step. In most real options settings, changes might not occur every day or every week or even every month, but probably do occur at least once every quarter. Moreover, such meticulous realism quickly lands the practitioner in other problems. In most real options settings, the research, analysis, and reporting protocols by which new information affecting project NPV becomes available occur at predetermined, non-random intervals, whereas the CP model is only valid for events occurring at random intervals. The order of changes in NPV is irrelevant. It is the size distribution of changes that matters, not when they occur. The frequency domain is what matters, not the time evolution.] -->
<!-- # It's ok to be lognormal -->
<!-- When taking a real options approach to project evaluation, it becomes necessary to think carefully about the evolution of project NPV over the life of the project. -->
<!-- Many consider the assumption of lognormally distributed NPV unrealistic. This is to some degree rooted in the original financial context, where the assumption of lognormal security returns is widely viewed as naive... -->
<!-- Bibby and Sorensen [@bibby1996hyperbolic] -->
<!-- Pennings and Lint []. .  misconception [Tankov]. -->
# No premise for risk-neutral valuation in far-from-market real options contexts

Another major source of complexity resulting in low adoption of ROV thinking regards the confused interpretation, in real options contexts, of the financial artifact known as "risk-neutral valuation".

The derivation of equation \ref{eq:rov} in the Appendix via the method of straightforward integration is atypical of the ROV literature. Most authors instead cite the Black-Scholes partial differentiation equation [-@black1973valuation] as the source of the ROV formula. The method of straightforward integration is followed here because it is considerably simpler than the Black-Scholes approach, and is stripped of financial trappings. In financial contexts, the Black-Scholes approach is advantageous because it generates a whole class of functional forms known as the "financial derivatives", of which the European call option formula (the financial analogue to equation \ref{eq:rov}) is just one.

More importantly, the Black-Scholes approach reveals, as a by-product, the deep result known as the principle of risk-neutral valuation: If it is not possible to make risk-free profits above the risk-free rate of return (the "no-arbitrage" rule), then investors are risk-neutral, i.e. "investors do not increase the expected return they require from an investment in order to compensate for increased risk" [@hull9thEdition]. Mathematically, this means that the $m$ in equation \ref{eq:rov} must be set equal to $r$ (where $r$, in financial contexts, refers to the risk-free rate of return).

The principle of risk-neutral valuation rests squarely upon the no-arbitrage rule, which is enforced through the market. This is fine in the financial context. However, in real options contexts, there is no clear theoretical or empirical basis for the no-arbitrage rule. Real project NPV is not a traded good, and there is no clear mechanism that might serve as a market analogue. On the contrary, most ROV contexts, especially research contexts, may be characterized as far-from-market---or even market failures, which the underlying project is supposed to redress. Empirically, it is a matter of public knowledge that project donors and managers are generally not risk-neutral; i.e., they would require an increase in expected project NPV to justify funding for a project with increased risk.

Critics note that the ROV literature is silent and/or conflicted on this point [@borison2005real; @block2007real]. It is not uncommon for ROV studies, and even ROV introductory texts, to assume risk-neutral valuation without any justification (see, for example, Trigeorgis [-@trigeorgis1993real], Majd and Pindyck [-@majd1987time], or Kemna [-@kemna1993case]). Some studies acknowledge the invalidity of the no-arbitrage argument in ROV contexts, but instead invoke "complete markets" to justify risk-neutral valuation (see, for example, Pennings and Lint [-@pennings1997option]). However, the complete markets assumption implies that 1) project NPV can be simulated by a portfolio of traded securities, and hence project risk can be hedged away by buying and/or selling these traded securities; and 2) project managers actually engage in the buying and selling of securities necessary to achieve this hedge. One may say without controversy that neither (1) nor (2) are common features of most far-from-market project management landscapes.

Mathematically, the absence of any empirical or theoretical premise for risk-neutral valuation in far-from-market real options contexts means that there is no reason to set $m$ in equation \ref{eq:rov} equal to $r$.

## The far-from-market Black-Scholes PDE

To see this, recall that, if $x(t)$ is a gBm, then, by Ito's lemma, the evolution of a function $f(x, t)$ is described as follows.

\begin{equation}
\Delta f = \left( \frac{\partial f}{\partial x} m x + \frac{\partial f}{\partial t} + \frac{1}{2} \frac{\partial^2 f}{\partial x^2} s^2 x^2 \right) \Delta t + \frac{\partial f}{\partial x} s x \epsilon \sqrt{\Delta t}
\label{eq:itoLem}
\end{equation}

(See Hull [-@hull9thEdition] for details.)

Black and Scholes [-@black1973valuation] famously noted that equations \ref{eq:gbmEq} and \ref{eq:itoLem} could be combined so as to eliminate the random term as follows.

\begin{equation}
\Delta f - \frac{\partial f}{\partial x} \Delta x = \left(\frac{\partial f}{\partial t} + \frac{1}{2} \frac{\partial^2 f}{\partial x^2} s^2 x^2 \right) \Delta t
\label{eq:bsInsight1}
\end{equation}

In the financial context, the lefthand side of this equation may be thought of as the instantaneous evolution over the increment $\Delta t$ of a portfolio long one share of the financial derivative $f$ and short a quantity $\partial f / \partial x$ of the underlying security $x(t)$. This is where Black and Scholes applied their no-arbitrage argument: Since the random---i.e. risky---term has been eliminated from equation \ref{eq:bsInsight1}, then the profit or loss of this portfolio over the increment $\Delta t$ must be riskless. That is, it must be equal to the starting value of the portfolio times the risk free rate ($r$).

\begin{equation}
\Delta f - \frac{\partial f}{\partial x} \Delta x = \left( f - \frac{\partial f}{\partial x} x \right) r \Delta t
\end{equation}

Equating the righthand side of this equation with the righthand side of the previous equation, the $\Delta t$'s cancel, resulting in the Black-Scholes partial differentiation equation (PDE).

\begin{equation}
\left( f - \frac{\partial f}{\partial x} x \right) r = \frac{\partial f}{\partial t} + \frac{1}{2} \frac{\partial^2 f}{\partial x^2} s^2 x^2 \end{equation}

Black and Scholes solved this PDE under the boundary condition $f(T) = \max(x(T) - K, 0)$, resulting in an expression for $f$ identical to the one derived above in equation \ref{eq:EmaxTK}, with the exception that $m$ is replaced with $r$.

But Black and Scholes' insight can be decomposed into two consecutive insights. The first insight is that, by eliminating the random terms in equation \ref{eq:bsInsight1}, the resulting expression is deterministic. And it equates a composite evolution in terms of $\Delta f$ and $\Delta x$ on the lefthand side to a time evolution $\Delta t$ on the righthand side. Regardless of the no-arbitrage rule, it follows trivially that

\begin{equation}
\Delta f - \frac{\partial f}{\partial x} \Delta x = \left( f - \frac{\partial f}{\partial x} x \right) \kappa \Delta t
\end{equation}

<!-- So long as -->

<!-- \begin{equation} -->
<!-- \kappa = \frac{\frac{\partial f}{\partial t} + \frac{1}{2} \frac{\partial^2 f}{\partial x^2} s^2 x^2}{f - \frac{\partial f}{\partial x} x} -->
<!-- \end{equation} -->

<!-- holds. -->
Where $\kappa$ is constant with respect to the interval $\Delta t$.

The second insight is about resolving the value of $\kappa$. In financial markets, the no-arbitrage rule requires that $\kappa = r$. In the absence of the no-arbitrage rule, however, the $r$ in the Black-Scholes PDE must be replaced by $\kappa$. Solving this non-market version of the Black-Scholes PDE at the boundary condition $f(T) = \max(x(T) - K, 0)$ and comparing it to equation \ref{eq:EmaxTK}, which is obtained through straightforward integration, reveals that $\kappa$ must default to $m$. In the absence of the no-arbitrage rule, then, the Black-Scholes PDE must default to

\begin{equation}
\left( f - \frac{\partial f}{\partial x} x \right) m = \frac{\partial f}{\partial t} + \frac{1}{2} \frac{\partial^2 f}{\partial x^2} s^2 x^2
\label{eq:ffmBSpde}
\end{equation}

This is the relevant version of the Black-Scholes PDE in far-from-market real options contexts.
<!-- the now deterministic equation means that the evolution in terms of $\Delta f$ and $\Delta x$ on the lefthand side of equation \ref{eq:bsInsight1} may be rewritten in terms of a time evolution $\Delta t$. That is, we have an equation of the following form. -->
<!-- the evolution of the lefthand side must be constant over the interval $\Delta t$. This means -->
<!-- \begin{equation} -->
<!-- a \Delta f - b \Delta x = c \Delta t -->
<!-- \end{equation} -->
<!-- [Where $a$ ...] -->
<!-- Regardless of the no-arbitrage argument, it follows trivially that -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- a f \kappa \Delta t - b x \kappa \Delta t  &= c \Delta t \\ -->
<!-- (a f - b x) \kappa \Delta t  &= c \Delta t -->
<!-- \end{split} -->
<!-- \end{equation} -->
<!-- And hence, trivially, -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- a f \kappa \Delta t - b x \kappa \Delta t  &= c \Delta t \\ -->
<!-- (a f - b x) \kappa \Delta t  &= c \Delta t -->
<!-- \end{split} -->
<!-- \end{equation} -->

# A closed form expression for $n$-fold real option value

Now we are in a position to derive the $n$-fold real option value formula. To begin, consider the far-from-market Black-Scholes PDE (equation \ref{eq:ffmBSpde}) for the 1-fold real option value function $f_1$, rearranged as an expression for $m f_1$.

<!-- The derivation consists of two steps. First we show that the $n$-fold real option value $f_n$ is lognormally distributed if project NPV $x(t)$ is lognormally distributed. Then, we show A closed form expression for $f_n$ then follows from equation \ref{eq:part1}. -->

\begin{equation}
m f_1 = \frac{\partial f_1}{\partial x} x m + \frac{\partial f_1}{\partial t} + \frac{s^2 x^2}{2} \frac{\partial^2 f_1}{\partial x^2}
\end{equation}

And consider Ito's lemma for $f_1$.

\begin{equation}
\Delta f_1 = \left( \frac{\partial f_1}{\partial x} m x + \frac{\partial f_1}{\partial t} + \frac{1}{2} \frac{\partial^2 f_1}{\partial x^2} s^2 x^2 \right) \Delta t + \frac{\partial f_1}{\partial x} s x \epsilon \sqrt{\Delta t}
\end{equation}

Note that the righthand side of the previous expression is equal to the first term in parentheses on the righthand side of Ito's lemma. Ito's lemma for $f_1$ may thus be rewritten as follows.

\begin{equation}
\Delta f_1 = m f_1 \Delta t + s x \frac{\partial f_1}{\partial x} \epsilon \sqrt{\Delta t}
\label{eq:itoLemf1}
\end{equation}

The second term on the righthand side can also be rewritten as follows.

\begin{equation}
s x \frac{\partial f_1}{\partial x} \epsilon \sqrt{\Delta t} = s f_1 \eta_{1, 0} \epsilon \sqrt{\Delta t}
\end{equation}

Where $\eta_{1, 0}$ is the elasticity of $f_1$ with respect to the project NPV $x$. That is, defining $f_0 = x$,

\begin{equation}
\begin{split}
\eta_{n, 0} &= \frac{f_0}{f_n} \frac{\partial f_n}{\partial f_0} = \frac{\partial \ln(f_n)}{\partial \ln(f_0)} \\
&= \frac{1}{100} \frac{%\Delta f_n}{%\Delta f_0}
\end{split}
\end{equation}

Equation \ref{eq:itoLemf1} can thus be rewritten

\begin{equation}
\Delta f_1 = m f_1 \Delta t + s f_1 \eta_{1, 0} \epsilon \sqrt{\Delta t}
\label{eq:gbmf1}
\end{equation}

By which it follows that $\Delta f_1 / f_1$ is normally distributed with mean $m \Delta t$ and variance $s^2 \eta_{1, 0}^2 \Delta t$. From this, it follows that $f_1$ is a geometric Brownian movement, such that $\ln(f_1)$ is normally distributed with mean $\ln(f_1(\hat{t})) + \left(m - \frac{s^2 \eta_{1, 0}^2}{2} \right) \tau_1$ and variance $s^2 \eta_{1, 0}^2 \tau_1$.

It follows that a 2-fold real option value function $f_2(f_1, t)$ also has a far-from-market Black-Scholes PDE.

\begin{equation}
\left( f_2 - \frac{\partial f_2}{\partial f_1} f_1 \right) m = \frac{\partial f_2}{\partial t} + \frac{1}{2} \frac{\partial^2 f_2}{\partial f_1^2} s^2 f_1^2
\end{equation}

With corrresponding Ito's lemma

\begin{equation}
\Delta f_2 = \left( \frac{\partial f_2}{\partial f_1} m f_1 + \frac{\partial f_2{\partial t} + \frac{1}{2} \frac{\partial^2 f_2}{\partial f_1^2} s^2 f_1^2 \right) \Delta t + \frac{\partial f_2}{\partial f_1} s f_1 \epsilon \sqrt{\Delta t}
\end{equation}

Such that

\begin{equation}
\begin{split}
\Delta f_2 &= m f_2 \Delta t + s f_2 \eta_{1, 0}  \frac{\partial f_2}{\partial f_1} \epsilon \sqrt{\Delta t} \\
&= m f_2 \Delta t + s f_2 \eta_{2, 0} \epsilon \sqrt{\Delta t}
\end{split}
\label{eq:gbmf2}
\end{equation}

By which it follows that $\Delta f_2 / f_2$ is normally distributed with mean $m \Delta t$ and variance $s^2 \eta_{2, 0}^2 \Delta t$; and that $\ln(f_2)$ is normally distributed with mean $\ln(f_2(\hat{t})) + \left(m - \frac{s^2 \eta_{2, 0}^2}{2} \right) \tau_2$ and variance $s^2 \eta_{2, 0}^2 \tau_2$. That is, $f_2$ is also a geometric Brownian movement.

And so on for $f_3$, $f_4$, $\dots$, $f_n$, there exists a far-from-market Black-Scholes PDE

\begin{equation}
\left( f_n - \frac{\partial f_n}{\partial f_{n - 1}} f_{n - 1} \right) m = \frac{\partial f_n}{\partial t} + \frac{1}{2} \frac{\partial^2 f_n}{\partial f_{n - 1}^2} s^2 f_{n - 1}^2
\end{equation}

Which, when combined with the corresponding Ito's lemma, results in an expression for $\Delta f_n$ as a geometric Brownian movement.

\begin{equation}
\Delta f_n = m f_n \Delta t + s f_n \eta_{n, 0} \epsilon \sqrt{\Delta t}
\label{eq:gbmfn}
\end{equation}

Such that $\Delta f_n / f_n$ is normally distributed with mean $m \Delta t$ and variance $s^2 \eta_{n, 0}^2 \Delta t$, and $\ln(f_n)$ is normally distributed with mean $\ln(f_n(\hat{t})) + \left(m - \frac{s^2 \eta_{n, 0}^2}{2} \right) \tau_n$ and variance $s^2 \eta_{n, 0}^2 \tau_n$.

Remarkably, then, the mean percentage change in $n$-fold option value is equal to the mean percentage change in project value. The variance of percentage changes in $n$-fold option value similarly has the parameter $s$ in common with the variance of percentage changes in project NPV, but differs from the latter by a factor of $\eta_{n,0}$, which changes in each time step.

Since $f_n$ is lognormally distributed, then, by equation \ref{eq:part1},

\begin{equation}
f_n = e^{(m - r) \tau_n} f_{n - 1}(\hat{t}) \Phi(\delta_n + s \eta_{n - 1, 0} \sqrt{\tau_n}) - e^{-r \tau_n} K_n \Phi(\delta_n)
\label{eq:rovN}
\end{equation}

Where

\begin{equation}
\delta_n = \frac{\ln \left(\frac{f_{n - 1}(\hat{t})}{K_n} \right) + \left(m - \frac{s^2 \eta_{n - 1, 0}^2}{2} \right) \tau_n}{s \eta_{n - 1, 0} \sqrt{\tau_n}}
\end{equation}

Which may be easily generalized to include abandonment value $B_n$ by following the steps in section \ref{sec:abandVal}.
<!-- equation \ref{eq:rovB}, -->

\begin{equation}
f_n = e^{(m - r) \tau_n} f_{n - 1}(\hat{t}) \Phi(\delta_n + s \eta_{n - 1, 0} \sqrt{\tau_n}) - e^{-r \tau_n} (K_n \Phi(\delta_n) - B_n ( 1 + \Phi(\delta_n)))
\label{eq:rovBn}
\end{equation}

Where

\begin{equation}
\delta_n = \frac{\ln \left(\frac{f_{n - 1}(\hat{t})}{K_n - B_n} \right) + \left(m - \frac{s^2 \eta_{n - 1, 0}^2}{2} \right) \tau_n}{s \eta_{n - 1, 0} \sqrt{\tau_n}}
\end{equation}

In the case of $n = 1$, it is easy to see that this reduces to the 1-fold real option value formula in equation \ref{eq:rovB}.

For multi-stage projects ($n > 1$), equation \ref{eq:rovBn} must be evaluated recursively as follows:

For $i = 1$ up to $i = n$,

1) Using $f_{i - 1}$ and $\eta_{i - 1, 0}$, evaluate $f_i$;

2) Using $f_i$, $f_{i - 1}$, and $\eta_{i - 1, 0}$, evaluate $\eta_{i, i - 1}$;

3) Evaluate $\eta_{i, 0} = \eta_{i, i - 1} \eta_{i - 1, 0}$;

(Remember that $f_0 = x$.)

The elasticity $\eta_{i, i - 1}$ works out to

\begin{equation}
\begin{split}
\eta_{i, i - 1} &= \frac{f_{i - 1}}{f_i} e^{(m - r) \tau_i} (\Phi(\delta_i + s \sqrt{\tau_i}) + f_{i - 1} \frac{\partial \eta_{i - 1, 0}}{\partial f_{i - 1}} \phi(\delta_i + s \sqrt{\tau_i})) \\
&= \frac{f_{i - 1}}{f_i} e^{(m - r) \tau_i} (\Phi(\delta_i + s \sqrt{\tau_i}) + (1 - \eta_{i - 1, 0}) \phi(\delta_i + s \sqrt{\tau_i}))
\end{split}
\end{equation}

In the case of $i = 1$, this reduces to

\begin{equation}
\frac{f_0}{f_1} \Phi(\delta_1 + s \sqrt{\tau_1})
\end{equation}

An R script of this algorithm is included in the Appendix.
<!-- And -->
<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \eta_{n - 1, 0} &= \frac{\partial \ln(f_n)}{\partial \ln(f_{n - 1})} = \frac{f_{n - 1}}{f_n} \frac{\partial f_n}{\partial f_{n - 1}} \\ -->
<!-- &= e^{(m - r) T_n} \frac{f_{n - 1}}{f_n} \left( \Phi_{n, 1} + f_{n - 1} \phi_{n, 1} \frac{\partial \eta_{n - 1, 0}}{\partial f_{n - 1}} s \sqrt{T_n} \right) \\ -->
<!-- &= \begin{cases} -->
<!-- 1, & n = 1 \\ -->
<!-- e^{(m - r) T_n} \frac{f_{n - 1}}{f_n} \left( \Phi_{n, 1} + f_{n - 1} \phi_{n, 1} \frac{\partial \eta_{n - 1, 0}}{\partial f_{n - 1}} s \sqrt{T_n} \right), & n > 1 -->
<!-- \end{cases} -->
<!-- \end{split} -->
<!-- \end{equation} -->
<!-- equations \ref{eq:gBmN} and \ref{eq:bsPDEn} imply that a function $f_{n + 1}$ of an underlying geometric Brownian motion $f_n$  -->
<!-- \begin{equation} -->
<!-- mf_{n + 1} = \frac{\partial f_n}{\partial f_{n - 1}} f_{n - 1} m + \frac{\partial f_n}{\partial t} + \frac{s^2 f_{n - 1}^2 \eta_{n - 1, 0}^2}{2} \frac{\partial^2 f_n}{\partial f_{n - 1}^2} -->
<!-- \label{eq:bsPDEn} -->
<!-- \end{equation} -->
<!-- note that by equation \ref{eq:bsPDEn},  -->

# An illustrative example of real option valuation of an AR4D project

Below I apply the $n$-fold option value model derived above to valuate a real AR4D project currently in implementation. The aim of the project is to develop a transgenic potato variety with resistance to Late Blight disease (henceforth LBr potato), for release as a public good in one or more developing countries. The LBr potato project follows the 4 stage structure described in section \ref{sec:resStages}. The costs and time duration associated with each stage are summarized in Table 1, based on figures documented in Schiek et al. [-@schiek2016demys].

Late Blight is the disease behind the infamous Irish Potato Famine of 1845-1849; and continues to pose a major threat to food security, especially in the developing world [@]. Successful research and development of LBr potato varieties adapted to local environments is thus a potentially very high reward, disruptive proposition. It is also a high risk proposition, facing numerous research and non-research related challenges, not least of which is a strong "anti-GM" lobby in many of the target populations.
<!-- Costs are assessed as the sum of staff costs, direct operating costs (including lab bench costs), indirect operating costs (overhead), external contract costs, and stewardship costs. -->

The donor, USAID, originally awarded the LBr potato research contract to Cornell University in 2010 (?), with planned release in India. However, stage 1 research and development at Cornell was unsuccessful. In 2015, USAID transferred the contract to Michigan State University (MSU), which proposed a different stage 1 research strategy, with planned release in Bangladesh and Indonesia instead of India. India was dropped as a target country partly because of the vigorous anti-GM lobby there, which made stage 4 success unlikely. MSU completed stage 1 successfully in 2019 (?), and USAID has recently awarded them a new contract for stage 2 research and development.
<!-- USAID-ABSP II program -->
<!-- The project occurred in 4 distinct stages: 1) Late Blight resistant (LBr) event production and selection, 2) Wide area testing, 3) Compilation of the regulatory dossier, and 4) Registration and regulatory affairs. -->
<!-- In stage 1, the lead gene construct was developed in a target potato variety, and a small number of transgenic events exhibiting high resistance to Late Blight, as well as a number of other qualities (absence of backbone vector sequences, minimum number copy number of R genes, etc.) were screened from a large number of explants. -->
<!-- In stage 2, the transgenic events selected in stage 1 were cultivated in three distinct locations and three distinct seasons to assess environmental effects on the LBr trait, and vice versa. Any impacts of LBr on other key traits such as yield, maturation time, taste, and so forth, were also assessed. --><!-- In stage 3, the regulatory dossier was compiled for submission to the National Competent Authority in the country where the LBr potato was to be released. The dossier included compositional and safety assessments, as well as the environmental assessments of the previous stage. -->
<!-- In stage 4, the regulatory dossier was defended and amended before the National Competent Authorty in the target country. This stage may involve a variety of activities, including public advocacy, lobbying, and submission of additional information. -->
<!-- For details see Schiek et al. [-@]. -->

```{r, echo = FALSE}
# Define functions
#----------------------------------------------------------------------------
# Table formatting function
FitFlextableToPage <- function(ft, pgwidth = 6){
  
  ft_out <- ft %>% autofit()
  #these_colWidths <- dim(ft_out)$widths * pgwidth  / (flextable_dim(ft_out)$widths)
  these_colWidths <- dim(ft_out)$widths
  these_colWidths[5] <- these_colWidths[1] * 1.5
  #these_colWidths[6] <- these_colWidths[6] * 0.7
  these_colWidths <- these_colWidths * pgwidth  / (flextable_dim(ft_out)$widths)
  ft_out <- width(ft_out, width = these_colWidths)
  return(ft_out)
}
#----------------------------------------------------------------------------
# N-fold ROV w/abandonment value function
rovN <- function(X0, Kvec, Bvec = NULL, Tvec, s, m, r){
  N <- length(Kvec)
  if(is.null(Bvec)){Bvec <- rep(0, N)}
  fNm1 <- X0
  etaNm10 <- 1
  etaVec <- c()
  fnVec <- c()
  sNm1vec <- c()
  Phi2vec <- c()
  Phi1vec <- c()
  for(i in 1:N){
    Kn <- Kvec[i]
    Bn <- Bvec[i]
    #Tn <- Tvec[i]
    Tn <- rev(cumsum(rev(Tvec)))[i]
    dn <- (log(fNm1 / (Kn - Bn)) + (m - s^2 * etaNm10^2 / 2) * Tn) / (s * etaNm10 * sqrt(Tn))
    #print(dn)
    # print(log(fNm1 / Kn))
    # print((s * etaNm10 * sqrt(Tn)))
    Phi1 <- pnorm(dn + s * etaNm10 * sqrt(Tn))
    Phi2 <- pnorm(dn)
    fn <- exp((m - r) * Tn) * fNm1 * Phi1 - exp(-r * Tn) * Kn * Phi2 + exp(-r * Tn) * Bn * (1 + Phi2)
    fnVec[i] <- fn
    etaVec[i] <- etaNm10
    sNm1vec[i] <- s * etaNm10
    Phi2vec[i] <- Phi2
    Phi1vec[i] <- Phi1
    if(i != N){
      phi1 <- dnorm(dn + s * etaNm10 * sqrt(Tn))
      dfndfNm1 <- exp((m - r) * Tn) * (Phi1 + phi1 * (1 - etaNm10) * s * sqrt(Tn))
      etaNnM1 <- fNm1 / fn * dfndfNm1
      etaNm10 <- etaNnM1 * etaNm10
      fNm1 <- fn
    }
  }
  underVec <- c(X0, fnVec[-N])
  df_out <- data.frame(Stage = rev(1:N),
                       OVn = fnVec, fnM10 = underVec,
                       KnM1 = Kvec, etaNm10 = etaVec,
                       sNm1 = sNm1vec, Phi2 = Phi2vec, Phi1 = Phi1vec)
  return(df_out)
  
}

#============================================================================
#============================================================================
# End function definition
#============================================================================
#============================================================================
# Project stage time durations and costs
# (Values given in chronologically reverse order, starting with stage n.)
# Kvec[1] is the launch cost, Tvec[1] is the launch duration.
# Time durations given in years, multiplied by 4 to convert to quarters.
# MSU/cornell
# Tvec <- c(2, 3.5, 1, 2, 4) * 4
# Kvec <- c(300000, 200000, 400000, 400000, 530000)
Tvec <- c(2, 3, 0.85, 1.7, 3.4) * 4
Kvec <- c(250000, 181000, 312000, 336000, 530000)
# Kvec <- c(180541, 311974, 335530, 530250)
# CIP
# Kvec <- c(52000, 213000, 396000, 929000)
# Tvec <- c(1, 0.25, 2, 4.75) * 4
#----------------------------------------------------------------------------
# Create table summarizing stage costs, time durations, and descriptions
stage1desc <- "Basic replication and scaling up"
stage2desc <- "Multi-location/season testing"
stage3desc <- "Compilation of the regulatory dossier"
stage4desc <- "Deregulation"
launchDesc <- "Launch in 2 countries"
descVec <- c(stage1desc, stage2desc, stage3desc, stage4desc, launchDesc)

df_table <- data.frame(Stage = c(1:4, "Launch"),
                       Kn = rev(Kvec),
                       Duration = rev(Tvec),
                       DurationCum = cumsum(rev(Tvec)),
                       Description = descVec)

colnames(df_table)[2:4] <- c("Cost (USD)", "Duration (quarters)", "Cumulative duration")

df_table <- regulartable(df_table)
df_table
#df_table <- width(df_table, width = 0.7)
df_table <- FitFlextableToPage(df_table)

table_title <- "Table 1: Project to research and develop Late Blight resistant potato for release as a public good in 2 developing countries."
#---------------------------------------------------------------------------
# Set abandonment value to 0 for starters
# Separate out the stage 4 cost and total time horizon less launch time
Bvec <- c(0, 0, 0, 0)
n <- length(Kvec) - 1
K1 <- Kvec[n + 1]
Tn <- sum(Tvec[-1])
#---------------------------------------------------------------------------
# Crude project NPV estimate loosely based on expected adoption rates, time horizons, etc.
rYrly_discrete <- 0.12 #0.035
rQtly_discrete <- (1 + rYrly_discrete)^(1 / 4) - 1
Tadopt <- 20 * 4
Timpact <- 10 * 4
yrlyBen <- 1 * 10^6 # (Assuming very low adoption)
x0 <- sum(yrlyBen / (1 + rQtly_discrete)^(Tn + Tadopt + 1:Timpact))
# (Override)
x0 <- 1 * 10^6
#---------------------------------------------------------------------------
# Convert discrete discount rate to continuous discount rate
r <- log(1 + rQtly_discrete)
#----------------------------------------------------------------------------
# Derive s and m
# Define confidence interval of interest, usually 95% (z = 1.96)
zBound <- 1.96
# Max and min log return (pctg error in project NPV) elicited from experts/stakeholders:
yMax <- 2.55 #1.31
yMin <- -0.85 #-0.88
# bTerm <- -(2 * yMin + 8 / 3 * zBound^2)
# yMax <- 1 / 2 * (-bTerm - sqrt(bTerm^2 + 4))
# Back out s and m
s <- (yMax - yMin) / (2 * zBound * sqrt(Tn))
m <- 1 / (2 * Tn) * (yMax + yMin + (yMax - yMin)^2 / (4 * zBound^2))
m - r
# Make sure m > r
#----------------------------------------------------------------------------
# Plausibility check:
# Coefficient of variation
cv <- round(s / (m * sqrt(Tn)), 2)
cv
# CV graph
# yMaxVec <- seq(0.1, 2.5, length.out = 35)
# sVec <- (yMaxVec - yMin) / (2 * zBound * sqrt(Tn))
# mVec <- 1 / (2 * Tn) * (yMaxVec + yMin + (yMaxVec - yMin)^2 / (4 * zBound^2))
# cvVec <- sVec / (mVec * sqrt(Tn))
# plot(yMaxVec, cvVec)
# cvVec <- 1 / (4 * zBound * Tn^2) * (yMaxVec^2 - yMin^2 + (yMaxVec - yMin)^3 / (4 * zBound^2))
# E[x(T)]|t=0
ExT <- exp(m * Tn) * x0
ExTdisc <- exp(-r * Tn) * ExT
# Expected log return
mu <- round((yMax + yMin) / 2, 2)
#m <- (mu / Tn + s^2 / 2)
#--------------------------------------------------------------------------
# Compare to conventional appraisal
#NPV <- exp(-r * sum(Tvec)) * (X0 - sum(Kvec))
conv <- exp((m - r) * Tn) * x0 - sum(exp(-r * rev(cumsum(rev(Tvec[-1])))) * (Kvec[-(n + 1)] - Bvec))
dif_conv <- conv - K1
#--------------------------------------------------------------------------
# Calculate n-fold ROV, at first with no abandonment value
df <- rovN(x0, Kvec[-n - 1], Bvec, Tvec[-1], s, m, r)
ROV <- df$OVn[nrow(df)]
dif <- ROV - K1
Phi2 <- round(df$Phi2[n], 2)
etaNm10 <- round(df$etaNm10[n], 2)
#--------------------------------------------------------------------------
# Calculate n-fold ROV, now with abandonment value
Bvec <- c(0, 0, 0, 100000)
df <- rovN(x0, Kvec[-n - 1], Bvec, Tvec[-1], s, m, r)
ROV_wB <- df$OVn[nrow(df)]
dif_wB <- ROV_wB - K1
Phi2wB <- round(df$Phi2[n], 2)
etaNm10_wB <- round(df$etaNm10[n], 2)
#--------------------------------------------------------------------------
# Compare to conventional appraisal with abandonment value
#NPV <- exp(-r * sum(Tvec)) * (X0 - sum(Kvec))
conv <- exp((m - r) * Tn) * x0 - sum(exp(-r * rev(cumsum(rev(Tvec[-1])))) * (Kvec[-(n + 1)] - Bvec))
dif_conv_wB <- conv - K1
#--------------------------------------------------------------------------
# Define slack function for root finding function
slackfun <- function(x0, Kvec, Bvec, Tvec, s, m, r, thresh){
  
  df <- rovN(x0, Kvec, Bvec, Tvec, s, m, r)
  ROV <- df$OVn[nrow(df)]
  slack <- ROV - thresh
  return(slack)
  
}

# testFn <- function(x, a, b, thresh){
#   f <- a * x^2 - b
#   slack <- f - thresh
#   return(slack)
# }
# a <- 1
# b <- 2
# thresh <- 0
# xGuess <- sqrt(2)
# testFn(xGuess, a, b, thresh)
# thisInt <- c(0, 2)
# minInt <- min(thisInt)
# maxInt <- max(thisInt)
# testRoot <- rootSolve::uniroot.all(testFn,
#                                    thisInt,
#                                    minInt,
#                                    maxInt,
#                                    a = a,
#                                    b = b,
#                                    thresh = thresh)

#--------------------------------------------------------------------------
interval <- c(1.04, 1.08) * 10^6
out <- uniroot(slackfun,
        interval,
        Kvec = Kvec[-n - 1],
        Bvec = Bvec,
        Tvec = Tvec[-1],
        s = s,
        m = m,
        r = r,
        thresh = K1
        )
x0bEven <- out[[1]]
# x0guess <- 1.06 * 10^6 #x0
# slackfun(x0guess, Kvec[-n - 1], Bvec, Tvec[-1], s, m, r, thresh = K1)
# minInt <- min(interval)
# maxInt <- max(interval)
# mm <- m
# x0Root <- rootSolve::uniroot.all(slackfun,
#                        interval = interval,
#                        lower = minInt,
#                        upper = maxInt,
#                        trace = 2,
#                        Kvec = Kvec[-n - 1],
#                        Bvec = Bvec,
#                        Tvec = Tvec[-1],
#                        s = s,
#                        mm = mm,
#                        r = r,
#                        thresh = K1
#                        )
#--------------------------------------------------------------------------

```

In real options language, USAID bought a 4-fold option on LBr potato when it awarded the original contract to Cornell; but this option expired out of the money at the end of stage 1. USAID then effectively bought a second 4-fold option on LBr potato when it transferred the contract to MSU. The first stage of the 4-fold option has expired in the money, whereupon it has become a 3-fold option.

In the present exercise, I calculate the 4-fold real option value of the USAID LBr potato contract awarded to MSU in 2015. This is the value of USAID's option, but not obligation, to fund multi-location, multi-season confined field trials once stage 1 is complete.
<!-- This is for purely pedagogical purposes hypothetical etc. -->
<!-- using the algorithm provided at the end of the previous section.  -->

No published ex-ante impact assessments of LBr potato release in Bangladesh and Indonesia yet exist. For the purposes of this pedagogical exercise, say that MSU and/or USAID internally conducted ex-ante impact assessments indicating a project NPV of $\$$`r x0`, with a high degree of uncertainty ranging from $-$ `r 100 * yMin` $\%$ to $+$`r 100 * ymax`$\%$ of NPV. Based on these uncertainty bounds (and assuming project NPV follows a gBm), the project NPV volatility parameter $s$ and quarterly growth rate $m$ are deduced following the method explained in section \ref{sec:volEst}. These work out to $m = $ `r m` (`r round(((1 + m)^4 - 1), 1)` annually) and $s = $ `r s`.

As a plausibility check, the project NPV growth rate coefficient of variation and log return are calculated based on the deduced values for $m$ and $s$. The coefficient of variation works out to `r cv`, while the log return works out to `r mu`. This then implies an expected project NPV at the end of the project of $\$$`r ExT` USD, the present value of which is $\$$`r ExTdisc` USD.

The quarterly discount rate used in these calculations is `r r`, which works out to an annual discount rate of `r rYrly_discrete`. There are good reasons for using a much lower discount rate when appraising these kinds of projects [-@moore2004just], but this is the discount rate most commonly used by USAID, the donor in this example [].
<!-- For public projects The annual discount rate is set equal to a social discount rate of $0.035$, following the recommendation of Moore et al. [-@moore2004just], which implies a quarterly discount rate of $r = $`r r`.-->
<!-- (Alston and Norton acknowledged in 1995 that the treatment of risk in impact assessment models was "rudimentary and in need of further refinement" [-@Alston1995]. Unfortunately, this remains true today.) If ex-ante risk assessments are not available, then they can be elicited in the survey of domain experts. Project risk might be crowdsourced, for example, by asking survey participants to estimate the maximum, minimum, and most probable impact of each given project. With these three inputs, it is then straightforward to compute standard deviation on the basis of an assumed project impact probability density. (For example, the minimum and maximum could be interpreted as the bounds of the 95% confidence interval of a lognormal probability density, and the "most probable impact" could be interpreted as its mode. From this it is then straightforward to derive the standard deviation.) -->
<!-- Discount rate [@caplin2004social; @moore2004just; @harrison2010valuing] -->
<!-- In the default approach, this would look like... -->
<!-- \begin{equation} -->
<!-- e^{(m - r) T_2} x_0 - e^{-rT_2}K_2 - e^{-rT_1}K_1 -->
<!-- \end{equation} -->
<!-- Then, in general, -->
<!-- \begin{equation} -->
<!-- e^{(m - r) T_N} x_0 - e^{-rT_N}K_N - e^{-rT_{N - 1}}K_{N - 1} - \dots - e^{-rT_{1}}K_{1} -->
<!-- \end{equation} -->

```{r, echo=FALSE}


df_table <- df[, c("Stage", "OVn", "fnM10", "etaNm10", "sNm1", "Phi2")]
#df_tabwB <- df_wB[nrow(df_wB), c("OVn", "fnM10", "etaNm10", "sNm1", "Phi2")]
colnames(df_table)[2:ncol(df_table)] <- c("ROV", "Underlying", "Elasticity", "Standard dev.", "Phi 2")

df_table <- regulartable(df_table)
#df_table <- width(df_table, width = 0.7)
df_table <- FitFlextableToPage(df_table)
df_table

```
Based on the values of $x_0$, $m$, $s$, and $r$, and the research stage costs and time horizons in Table 1, the 4-fold option value of the LBr potato project (equation \ref{eq:rovBn}) works out to $\$$`r ROV` USD, which exceeds the stage 1 implementation cost ($\$$`r Kvec[n]`) by $\$$`r dif` (Table 2). Hence the investment is justified under the parameter settings defined above.

The conventional CBA approach (expression \ref{eq:npvIneqN2}), by contrast, works out to $\$$`r conv`, which falls below the stage 1 cost by $\$$`r -dif_conv`. On a conventional CBA basis, then, the LBr project would be rejected.

The ROV approach provides additional output, also reported in Table 2, that may be of use in decision making. The probability of stage 1 research expiring in the money ($\Phi(\delta_4)$) is an encouraging `r Phi2`, while the elasticity of the 4-fold ROV with respect to project NPV is a moderate `r etaNm10`, meaning that a $1\%$ change in project NPV results in a `r etaNm10`$\%$ change in the underlying.

Since the 4-fold option value is calculated recursively, the 3-fold, 2-fold, and 1-fold options are calculated and reported in the process, along with their respective probabilities of expiring in the money and elasticities with respect to project NPV. It may be of interest to note that the 3-fold, 2-fold, and 1-fold options are deep in the money with a probability of expiring in the money close to 1. This suggesting that funding of stages 2, 3, and 4 will be easy to justify if researchers can just manage to clear the stage 1 hurdle successfully.
<!-- `r round(dif)` -->

The exercise so far assumes that abandonment value is zero. Now consider the same exercise, but with a stage 1 abandonment value of $\$$`r Bvec[n]` USD, corresponding to the value of permanent upgrades to human and physical capital at the national agricultural research institutions of the target countries, such that this value will be realized whether or not stage 1 is successful. Then the 4-fold ROV works out to $\$$`r round(ROV_wB)` USD, and the probability of expiring in the money at the end of stage 1 increases to `r Phi2wB`.
<!-- `r round(dif_npv)` -->

When there is considerable uncertainty or disagreement surrounding project NPV, as in the present example, a better question to ask may be: what is the minimum project NPV required for the donor's investment to break even? That is to say, instead of calculating the $n$-fold ROV based on a dubious project NPV, it may be more useful to deduce the minimum project NPV required for the donor's investment to break even. This is achieved by solving the $n$-fold ROV model implicitly for $x_0$ such that $f_n - K_n = 0$. The audience can then draw its own conclusions about whether or not real project NPV is above or below this minimum threshold. In the present exercise, the break even project NPV works out to $\$$`r x0bEven`.

Point estimates such as this are of limited use when there is high uncertainty surrounding parameter settings. In such cases, it is better to report ROV in a map format spanning some relevant range of values of the most uncertain parameters. This allows decision makers to quickly discern ROV across a wide range of parameter values, as well as to develop a sense of how sharply ROV varies with these parameters. In other words, it combines ROV estimates and sensitivity analysis in a single reporting format. In the pedagogical example explored here, there is high uncertainty surrounding project NPV. A 3-fold ROV map spanning a range of combinations of upper and lower bounds on the 95% confidence interval is therefore provided in Figure \ref{fig:rovNmap}.

```{r, fig.show = "hold", fig.width = 4, fig.height=3, fig.align="left", fig.cap="\\label{fig:rovNmap}A map of 4-fold real option values across multiple combinations of project NPV minimum and maximum bounds.", echo = FALSE}

# ROV Map
nRes <- 20
#-----
this_r <- 0.04
#-----
# X0vec <- seq(0.5, 1.5, length.out = nRes)
# cvVec <- seq(2, 15, length.out = nRes)
yMaxVec <- seq(0.1, 3.5, length.out = nRes)
yMinVec <- seq(-0.9, -0.1, length.out = nRes)
#----
difmat <- matrix(NA, nRes, nRes)
difNPVmat <- matrix(NA, nRes, nRes)
phi2mat <- matrix(NA, nRes, nRes)
x0starMat <- matrix(NA, nRes, nRes)
for(i in 1:nRes){
    #x0 <- X0vec[i] * 10^6
    yMin <- yMinVec[i]
  for(j in 1:nRes){
    #cv <- cvVec[j]
    #s <- m * cv
    yMax <- yMaxVec[j]
# Back out s and m
s <- (yMax - yMin) / (2 * zBound * sqrt(Tn))
mu <- (yMax + yMin) / 2
m <- (mu / Tn + s^2 / 2)
#--------------------------------------------------------------------------
# Compare to conventional appraisal
#NPV <- exp(-r * sum(Tvec)) * (X0 - sum(Kvec))
conv <- exp((m - r) * Tn) * x0 - sum(exp(-r * rev(cumsum(rev(Tvec[-1])))) * (Kvec[-(n + 1)] - Bvec))
dif_conv <- conv - K1
#--------------------------------------------------------------------------
# Calculate n-fold ROV with no abandonment value
df <- rovN(x0, Kvec[-n - 1], Bvec, Tvec[-1], s, m, r)
ROV <- df$OVn[nrow(df)]
dif <- ROV - K1
Phi2 <- df$Phi2[n] #round(df$Phi2[n], 2)
etaNm10 <- round(df$etaNm10[n], 2)
#--------------------------------------------------------------------------
interval <- c(.4, 10) * 10^6
out <- uniroot(slackfun,
        interval,
        Kvec = Kvec[-n - 1],
        Bvec = Bvec,
        Tvec = Tvec[-1],
        s = s,
        m = m,
        r = r,
        thresh = K1
        )
x0star <- out[[1]]
#--------------------------------------------------------------------------
  difmat[i, j] <- dif
  phi2mat[i, j] <- Phi2
  difNPVmat[i, j] <- dif_conv
  x0starMat[i, j] <- x0star

    }
}

#--------------------------------------

colnames(difmat) <- as.character(yMaxVec)
row.names(difmat) <- as.character(X0vec)
df_plot <- reshape2::melt(difmat)
colnames(df_plot) <- c("Project NPV at t = 0 (million USD)", "y max", "ROV net benefit\n(100 thousand USD)")
colnames(phi2mat) <- as.character(yMaxVec)
row.names(phi2mat) <- as.character(X0vec)
df_plotPhi2 <- reshape2::melt(phi2mat)
colnames(df_plotPhi2) <- c("Project NPV at t = 0 (million USD)", "y max", "Probability of success")
#, "NPV net benefit", )

#---
gg <- ggplot(df_plot, aes(x = `y max`,
                          y = `Project NPV at t = 0 (million USD)`,
                          fill = `ROV net benefit\n(100 thousand USD)`))
                          #fill = `Probability of success`))
gg <- gg + geom_tile()
gg <- gg + scale_fill_gradient2(high = "green",
                                mid = "yellow",
                                low = "red",
                                midpoint = 0)
gg <- gg + theme_bw()
gg <- gg + theme(legend.position = "top",
                 axis.title.y = element_text(size = axisTitle_size),
                 axis.text.y = element_text(size = axisText_size),
                 axis.title.x = element_text(size = axisTitle_size),
                 axis.text.x = element_text(size = axisText_size),
                 legend.text = element_text(size = legendText_size),
                 legend.title = element_text(size = axisTitle_size))
gg_rovMap <- gg
#---------------------------------------------------------------------------
gg <- ggplot(df_plotPhi2, aes(x = `y max`,
                          y = `Project NPV at t = 0 (million USD)`,
                          fill = `Probability of success`))
gg <- gg + geom_tile()
gg <- gg + scale_fill_gradient(high = "green",
                                #mid = "yellow",
                                low = "red")#,
                                #midpoint = 0.5)
gg <- gg + theme_bw()
gg <- gg + theme(legend.position = "top",
                 axis.title.y = element_blank(),
                 axis.text.y = element_blank(),
                 axis.title.x = element_text(size = axisTitle_size),
                 axis.text.x = element_text(size = axisText_size),
                 legend.text = element_text(size = legendText_size),
                 legend.title = element_text(size = axisTitle_size))
gg_probMap <- gg
#---------------------------------------------------------------------------

gg_rovMap + gg_probMap + plot_layout(ncol = 2)


```

# Discussion and conclusion

New tools are urgently required to account for uncertainty and "price in" the donor's prerogative to discontinue funding of under-performing projects.
It stands to reason.. account for risk and price in donors' option to .Confidence in can be restored. Real options valuation offers a promising method to 

While AR4D is the motivating context for the present work, the results apply to any far-from-market real options context.

watch out for abuse

program level valuation - easier to justify the assumption of lognormal changes in every time step since the program level includes numerous projects at various stages of execution... including complementary activities..


applies to livestock projects as well

when m < r

<!-- Given the not-for-profit...some donors may be willing to fund projects even if the ROV falls somewhat below the stage 1 cost. -->

For probabilities close to 1, the ROV will differ little from the default NPV approach.

This is not a LBr ex-ante impact assessment paper.

Researchers resent being forced to place a value on impacts that may be 15-30+ years off [Moriarity paper, etc.]. The ROV approach offers them a way to accommodate this vast uncertainty (and their frustration at being forced to essentially take a shot in the dark) through the parameter $s$. Oftentimes, ROV differs meaningfully from NPV only when $s$ is many times greater than $m$. In the example presented here, $s$ is set to `r cv` times $m$.

For projects with high expected net payoff, the probability of project success $\Phi(\delta_N)$ will effectively be $1$---and hence ROV will not differ meaningfully from NPV---unless there is also high uncertainty surrounding the expected payoff.    In the example above [refer to decision map figure], the ratio is so high that project ROV is virtually the same as NPV for a coefficient of variation less than $5$. [The higher the ratio of benefits to costs, the higher must be the uncertainty for there to be a meaningful difference between ROV and NPV.]

It may thus make more sense to extend to the discovery or blue skies research phase, where the exact applications and target populations and environments of the technology may not be known at the start of the research, more chance of serendipity, i.e. of applications falling outside of this range of expectations. [The high iron bean story.]

If high reward projects are not also high risk, there is no point in assessing ROV, as the result will not differ significantly from NPV.

<!-- Likely that the project NPV $x(0)$ much greater than assumed here, in which case the probability of continuing with stage 2 would be close to $100%$, and hence ROV would essentially be the same as NPV. And would exceed the barrier cost by a wide margin. -->

# References {-}


<div id="refs"></div>

\pagebreak

# Appendix: Proof of equation \ref{eq:part1} through straightforward integration

<!-- Lemma: -->

<!-- If $v(t)$ follows a geometric Brownian motion; that is to say, if the instantaneous evolution of $v(t)$ over time can be expressed -->

<!-- \begin{equation} -->
<!-- \Delta v = \alpha v \Delta t + \beta v \epsilon \sqrt{\Delta t} \:\:;\:\:\: \Delta v = v(t + \Delta t) - v(t) -->
<!-- \label{eq:deltaV} -->
<!-- \end{equation} -->

<!-- Where $\epsilon$ is a normally distributed random variable with mean $0$ and variance $1$, and $\alpha$ and $\beta$ are constant with respect to the instantaneous time increment $\Delta t$, such that -->

<!-- \begin{equation} -->
<!-- \frac{\Delta v}{v} ~ \phi(\alpha \Delta t, \beta^2 \Delta t) -->
<!-- \end{equation} -->

<!-- Where $\phi()$ is the standard normal probability density function; and hence -->

<!-- \begin{equation} -->
<!-- v(T) = v(\hat{t}) e^{(\alpha - \beta^2 / 2) \tau + \beta \epsilon \sqrt{\tau}} -->
<!-- \label{eq:defVt} -->
<!-- \end{equation} -->

<!-- Where $T$ is some future time step, $\hat{t} < T$ is the time step at which $v(T)$ is evaluated, and $\tau = T - \hat{t}$. (See Hull [-@hull9thEdition] for details.) Then, for a constant C, -->
<!-- <!-- , such that --> 
<!-- <!-- \begin{equation} --> 
<!-- <!-- \ln \left(\frac{v(T)}{v(\hat{t})} \right) ~ \phi \left(\left(\alpha - \frac{\beta^2}{2} \right) \tau, \beta^2 \tau \right) -->
<!-- <!-- \end{equation} -->

<!-- \begin{equation} -->
<!-- E[max(v(T) - C, 0)]\bigr|_{t = \hat{t}} = e^{\alpha \tau} v(\hat{t}) \Phi(\delta + \beta \sqrt{\tau}) - C \Phi(\delta) -->
<!-- \end{equation} -->

<!-- Where $\Phi()$ is the standard normal cumulative distribution function, and -->

<!-- \begin{equation} -->
<!-- \delta = \frac{\ln(v(\hat{t}) / C) + (\alpha - \beta^2 / 2) \tau}{\beta \sqrt{\tau}} -->
<!-- \end{equation} -->

<!-- Proof: -->

(The proof below closely follows that of Hull in the appendix to chapter 15 of his book [-@hull9thEdition]. It is presented here solely for the reader's convenience, with no claim to originality.)
<!-- _Part 1_ -->

By definition, for a random variable $q$ and a constant $C$,

\begin{equation}
E[max(q - C, 0)] = \int_{C}^{\infty} (v - K) p(v) \: dv
\label{eq:def}
\end{equation}

Where $p(q)$ is the probability density function of the random variable $q$. If $\ln(q)$ is normally distributed with mean $\nu$ and variance $\omega^2$, then

\begin{equation}
E[q] = e^{\nu + \frac{\omega^2}{2}}
\label{eq:muXT}
\end{equation}

and

\begin{equation}
p(q) = \frac{1}{v \omega} \phi \left(\frac{\ln(q) - \nu}{\omega} \right)
\end{equation}

Now, introducing a change of variables,

\begin{equation}
u = \frac{\ln(q) - \nu}{\omega}
\label{eq:subThis1}
\end{equation}

Such that

\begin{equation}
\frac{du}{d q} = \frac{1}{q \omega}\: \rightarrow \: d q = q \omega du
\label{eq:subThis2}
\end{equation}

The definition in equation \ref{eq:def} can be rewritten as follows.

\begin{equation}
\begin{split}
E[max(q - C, 0)] &= \int_{\frac{\ln(C) - \nu}{\omega}}^{\infty} (e^{u \omega + \nu} - C) \phi(u) \: du \\
&= \int_{\frac{\ln(C) - \nu}{\omega}}^{\infty} e^{u \omega + \nu} \phi(u) \: du - C \int_{\frac{\ln(C) - \nu}{\omega}}^{\infty} \phi(u) \: du \\
&= \int_{\frac{\ln(C) - \nu}{\omega}}^{\infty} e^{u \omega + \nu} \phi(u) \: du - C \Phi \left(-\frac{\ln(C) - \nu}{\omega} \right)
\end{split}
\end{equation}

The remaining integral on the righthand side of the definition resolves as follows.

\begin{equation}
\begin{split}
\int_{\frac{\ln(C) - \nu}{\omega}}^{\infty} e^{u \omega + \nu} \phi(u) \: du &= \frac{1}{\sqrt{2 \pi}} \int_{\frac{\ln(C) - \nu}{\omega}}^{\infty} e^{u \omega + \nu -\frac{u^2}{2}} \: du \\
&= \frac{1}{\sqrt{2 \pi}} \int_{\frac{\ln(C) - \nu}{\omega}}^{\infty} e^{-1 / 2 (u^2 - 2 u \omega - 2 \nu)} \: du \\
&= \frac{1}{\sqrt{2 \pi}} \int_{\frac{\ln(C) - \nu}{\omega}}^{\infty} e^{-\frac{(u^2 - \omega)^2}{2} + \nu + \frac{\omega^2}{2}} \: du \\
&= \frac{1}{\sqrt{2 \pi}} \int_{\frac{\ln(C) - \nu}{\omega}}^{\infty} e^{-\frac{(u^2 - \omega)^2}{2}} e^{\nu + \frac{\omega^2}{2}} \: du \\
&= e^{\nu + \frac{\omega^2}{2}} \frac{1}{\sqrt{2 \pi}} \int_{\frac{\ln(C) - \nu}{\omega}}^{\infty} e^{-\frac{(u^2 - \omega)^2}{2}} \: du \\
&= \frac{E[x(T)]}{\sqrt{2 \pi}} \int_{\frac{\ln(C) - \nu}{\omega}}^{\infty} e^{-\frac{(u^2 - \omega)^2}{2}} \: du
\end{split}
\end{equation}

The definition may now be rewritten as follows.

\begin{equation}
E[max(q - C, 0)] = E[q] \Phi \left(- \frac{\ln(C) - \nu}{\omega} + \omega \right) - C \Phi \left(-\frac{\ln(C) - \nu}{\omega} \right)
\end{equation}

Note that \ref{eq:muXT} can be rearranged into an expression for $\nu$.

\begin{equation}
\nu = \ln(E[q]) - \omega^2 / 2
\label{eq:subThis1}
\end{equation}

Substituting this for $\nu$ in the definition gives

\begin{equation}
E[max(q - C, 0)] = E[q] \Phi \left(\frac{\ln(E[q] / C) + \omega^2 / 2}{\omega} \right) - C \Phi \left(-\frac{\ln(E[q] / C) - \omega^2 / 2}{\omega} \right)
\end{equation}
<!-- \label{eq:part1} -->

<!-- _Part 2_ -->

<!-- Let $q$ equal the geometric Brownian motion $v(t)$ defined in equation \ref{eq:defVt}, and note that -->

<!-- \begin{equation} -->
<!-- E[v(T)] \bigr|_{t = \hat{t}} = v(\hat{t}) e^{\alpha \tau} -->
<!-- \end{equation} -->

<!-- \begin{equation} -->
<!-- Var[\ln(v(T))] \bigr|_{t = \hat{t}} = \beta^2 \tau -->
<!-- \end{equation} -->
<!-- <!-- Where $\alpha = 1 / \tau \: E[\Delta v / v]$, and $\beta^2 = 1 / \tau \:Var[\Delta v / v]$. --> -->

<!-- (See Hull [-@hull9thEdition] for details.) -->
<!-- <!-- Substituting $v(T)$ for $q$, $E[v(T)]\bigr|_{t = \hat{t}}$ for $E[q]$, and $Var[\ln(v(T))] \bigr|_{t = \hat{t}}$ for $\omega$, --> -->

<!-- Then equation \ref{eq:part1} can be rewritten as follows. -->

<!-- \begin{equation} -->
<!-- E[max(v(T) - C, 0)]\bigr|_{t = \hat{t}} = v(\hat{t}) e^{\alpha \tau} \Phi \left(\frac{\ln(v(\hat{t}) / C) + (\alpha + \beta^2 / 2) \tau}{\beta \sqrt{\tau}} \right) - C \Phi \left(\frac{\ln(v(\hat{t}) / C) + (\alpha - \beta^2 / 2) \tau}{\beta \sqrt{\tau}} \right) -->
<!-- \end{equation} -->

<!-- Or, more compactly, -->

<!-- \begin{equation} -->
<!-- E[max(v(T) - C, 0)]\bigr|_{t = \hat{t}} = v(\hat{t}) e^{\alpha \tau} \Phi(\delta + s \sqrt{\tau}) - C \Phi(\delta) -->
<!-- \end{equation} -->

<!-- Where -->

<!-- \begin{equation} -->
<!-- \delta = \frac{\ln(v(\hat{t}) / C) + (\alpha - \beta^2 / 2) \tau}{\beta \sqrt{\tau}} -->
<!-- \end{equation} -->

<!-- Multiplying this by the discount factor then gives -->
<!-- \begin{equation} -->
<!-- e^{-r \tau} E[max(v(T) - C, 0)]\bigr|_{t = \hat{t}} = v(\hat{t}) e^{(\alpha - r) \tau} \Phi(\delta + \beta \sqrt{\tau}) - e^{-r \tau} C \Phi(\delta) -->
<!-- \end{equation} -->
$\blacksquare$
<!-- This is the ROV formula in equation \ref{eq:rov}. -->























<!-- >"[The proven LB resistance] technology is applied to the target [potato] variety in order to obtain a reduced number of candidate...transgenic events. Once the lead gene construct has been developed, a relatively large number of explants is screened by successive procedures in order to identify transgenic events from among the explants, remove the transgenic events with backbone vector sequences, select the transgenic events exhibiting high resistance to LB in confined field trials [CFTs], and select the transgenic events with the minimum copy number of R genes. The output of this process is a small number of candidate pre-commercial transgenic events selected for wide area testing, as well as molecular characterisation data to be used in the compilation of the regulatory dossier later on" []. -->

<!-- Stage 2: Wide area testing -->

<!-- >"The candidate...transgenic events are evaluated under normal and/or managed field conditions for resistance to LB. Depending on the diversity of the environment where the potato variety is grown in the target country, CFTs are conducted in multiple locations to assess any environmental effect on the trait performance. At the same time, the agronomic performance of the candidate pre-commercial transgenic events are assessed and compared to the non-transformed counterpart. This may include testing the number and kinds of fungicide spray needed to prevent productivity losses under exceptionally heavy disease pressure. These field trials also test for any negative impact of the trait on key performance attributes, yield or tuber quality, or potential negative environmental interactions. At the end of this process, one ...transgenic event is identified" []. -->

<!-- Stage 3: Compilation of the regulatory dossier -->

<!-- >"In this process, the best pre-commercial transgenic event (selected under [Stage] 2) is examined to ensure compliance with all regulatory requirements established by the National Competent Authority (NCA), and the corresponding regulatory dossier is compiled for submission to the NCA. Much of the data required for this examination have already been generated and collected under previous...[stages]. Therefore the respective costs assessed ...[here] are only those incurred in the processing, filing, and redaction of the results of the laboratory and the field observations for the regulatory dossier. Only the compositional assessment data and the safety assessment data (protein production and characterisation data for allergenicity and toxicity assessments) are generated and collected under Process 3. At the end of this process, a regulatory dossier is ready for submission to the NCA" []. -->

<!-- sub-processes: molecular characterisation data are generated under sub-process 1.5; while the environmental impact and phenotypic/agronomic data are generated under sub-processes 2.1 and 2.2. -->
<!-- Stage 4: Registration and regulatory affairs -->
<!-- >"Once the regulatory dossier has been submitted it must be defended, amended, and completed before the NCA authorises commercial production. This process may involve a variety of activities, including public advocacy, lobbying, and submission of additional information not included in the original dossier. In this study, it is assumed that any requests made by the NCA will concern existing information and data that were not included in the regulatory dossier, or data included in the regulatory dossier but not analysed using the methodology favored by the examiners, or not discussed at the level of details desired by the examiners. Hence, participating institutions assessed the cost and duration of this process assuming that the NCA makes its requests and decisions solely on scientific bases directly related to the regulatory dossier, and that it does not request further regulatory trials. The end product of this process is the authorization of commercial production of one transgenic LBr potato variety in one of the target  -->
<!-- countries" []. -->

<!-- The far-from-market Black-Scholes PDE can be rearranged into an expression for $fm$ as follows. -->


<!-- And, repeating the steps in equations \ref...\ref, -->

<!-- \begin{equation} -->
<!-- \ln \left(\frac{f}{f\bigr|_{t = 0}}  \right) ~ \phi \left(\left(m - \frac{s^2 \eta_{f, x}^2}{2} \right) T, s^2 \eta_{f, x}^2 T \right) -->
<!-- \end{equation} -->

<!-- The function $f$ of the geometric Brownian motion $x$ is thus itself a geometric Brownian motion. That is to say, the log returns of $f$ are normally distributed with mean $\left(m - \frac{s^2 \eta_{f, x}^2}{2} \right) T$ and variance $s^2 \eta_{f, x}^2 T$. -->

<!-- Now consider a function $g$ which represents the option value of the option value $f$. That is, -->

<!-- \begin{equation} -->
<!-- g = e^{-r T_g} E[\max(f(T_g) - K_f, 0)] -->
<!-- \end{equation} -->

<!-- Where $T_g$ is the time from the present moment ($t = 0$) until maturity of $g$ (the start of the subsequent real option $f$, if exercised), and $K_f$ is the cost of the stage associated with $f$ that is triggered if $f(T_g) > K_f$. -->

<!-- By Ito's Lemma, the evolution of $g$ may be expressed -->

<!-- \begin{equation} -->
<!-- \Delta g = \left( \frac{\partial g}{\partial f} f m + \frac{\partial g}{\partial t} + \frac{s^2 f^2 \eta_{f, x}^2}{2} \frac{\partial^2 g}{\partial f^2} \right) \Delta t + s f \varespsilon_{f, x} \frac{\partial g}{\partial f} \epsilon \sqrt{\Delta t} -->
<!-- \end{equation} -->

<!-- Subtracting $\partial g/\partial f \Delta f$ from $\Delta g$ and applying Black-Scholes insight #2 results in the following non-market Black-Scholes PDE for $g$. -->

<!-- \begin{equation} -->
<!-- mg = \frac{\partial g}{\partial f} f m + \frac{\partial g}{\partial t} + \frac{s^2 f^2 \eta_{f, x}^2}{2} \frac{\partial^2 g}{\partial f^2} -->
<!-- \label{eq:bsPDEg} -->
<!-- \end{equation} -->

<!-- So that $mg$ may be substituted for the first term on the righthand side of the previous equation, giving -->

<!-- \begin{equation} -->
<!-- \Delta g = m g \Delta t + s f \varespsilon_{f, x} \frac{\partial g}{\partial f} \epsilon \sqrt{\Delta t} -->
<!-- \end{equation} -->

<!-- And the second term may also be rewritten as follows. -->

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \Delta g &= m g \Delta t + s g \varespsilon_{g, f} \varespsilon_{f, x} \epsilon \sqrt{\Delta t} \\ -->
<!-- &= m g \Delta t + s g \varespsilon_{g, x} \epsilon \sqrt{\Delta t} -->
<!-- \end{split} -->
<!-- \label{eq:gbmg} -->
<!-- \end{equation} -->

<!-- By which it follows that -->

<!-- \begin{equation} -->
<!-- \frac{\Delta g}{g} ~ \phi(m \Delta t, s^2 \eta_{g, x}^2 \Delta t) -->
<!-- \end{equation} -->

<!-- And -->

<!-- \begin{equation} -->
<!-- \ln \left(\frac{g}{g \bigr|_{t = 0}}  \right) ~ \phi \left(\left(m - \frac{s^2 \eta_{g, x}^2}{2} \right) T, s^2 \eta_{g, x}^2 T \right) -->
<!-- \end{equation} -->

<!-- The option on the option, or "compound option", $g$ is thus also a geometric Brownian motion. -->

<!-- By now, the astute reader will have noticed a pattern, such that the geometric Brownian motions in equations \ref..., \ref{eq:gbmf}, and \ref{eq:gbmg} may be consolidated into a single equation, as follows. Letting $f_0 = x$, $f_1 = f$, and $f_2 = g$, -->

<!-- \begin{equation} -->
<!-- \Delta f_n = m f_n \Delta t + s \eta_{n, 0} \epsilon \sqrt{\Delta t} -->
<!-- \label{eq:gBmN} -->
<!-- \end{equation} -->

<!-- For $n = 0, 1, 2$. (Where, to be clear, $\eta_{n, 0} = \frac{\partial \ln(f_n)}{\partial \ln(f_0)}$.) -->

<!-- And the non-market Black-Scholes PDEs in equations \ref..., \ref{eq:bsPDEf}, and \ref{eq:bsPDEg} can likewise be consolidated as follows. -->

<!-- \begin{equation} -->
<!-- mf_n = \frac{\partial f_n}{\partial f_{n - 1}} f_{n - 1} m + \frac{\partial f_n}{\partial t} + \frac{s^2 f_{n - 1}^2 \eta_{n - 1, 0}^2}{2} \frac{\partial^2 f_n}{\partial f_{n - 1}^2} -->
<!-- \label{eq:bsPDEn} -->
<!-- \end{equation} -->

<!-- For $n = 1, 2$. To extend equations \ref{eq:gBmN} and {eq:bsPDEn} to $n > 2$, note that, since $f_n$ is a geometric Brownian motion, then, by Ito's Lemma, the evolution of a function $f_{n + 1}$ of $f_n$ can be expressed -->

<!-- \begin{equation} -->
<!-- \Delta f_{n + 1} = \left( \frac{\partial f_{n + 1}}{\partial f_n} f_n m + \frac{\partial f_{n + 1}}{\partial t} + \frac{s^2 f_n^2 \eta_{n, 0}^2}{2} \frac{\partial^2 f_{n + 1}}{\partial f_n^2} \right) \Delta t + s f_n \varespsilon_{n, 0} \frac{\partial f_{n + 1}}{\partial f_n} \epsilon \sqrt{\Delta t} -->
<!-- \label{eq:gbmNproof1} -->
<!-- \end{equation} -->

<!-- Subtracting $\partial f_{n + 1} / \partial f_n \Delta f_n$ from $\Delta f_{n + 1}$ and applying Black-Scholes insight #2 in the non-market context results in the non-market Black-Scholes PDE for $f_{n + 1}$. -->

<!-- \begin{equation} -->
<!-- mf_{n + 1} = \frac{\partial f_{n + 1}}{\partial f_n} f_n m + \frac{\partial f_{n + 1}}{\partial t} + \frac{s^2 f_n^2 \eta_{n, 0}^2}{2} \frac{\partial^2 f_{n + 1}}{\partial f_n^2} -->
<!-- \end{equation} -->

<!-- By which the previous equation reduces to -->

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \Delta f_{n + 1} &= m f_{n + 1} \Delta t + s f_n \varespsilon_{n, 0} \frac{\partial f_{n + 1}}{\partial f_n} \epsilon \sqrt{\Delta t} \\ -->
<!-- &= m f_{n + 1} \Delta t + s f_{n + 1} \varespsilon_{n, 0} \varespsilon_{n + 1, n} \epsilon \sqrt{\Delta t} \\ -->
<!-- &= m f_{n + 1} \Delta t + s f_{n + 1} \varespsilon_{n + 1, 0} \epsilon \sqrt{\Delta t} -->
<!-- \end{split} -->
<!-- \label{eq:gbmNproof3} -->
<!-- \end{equation} -->

<!-- By which it follows that -->

<!-- \begin{equation} -->
<!-- \frac{\Delta f_{n + 1}}{f_n} ~ \phi(m \Delta t, s^2 \eta_{f_{n + 1}, f_n}^2 \Delta t) -->
<!-- \label{eq:gBmNp1} -->
<!-- \end{equation} -->

<!-- And -->

<!-- \begin{equation} -->
<!-- \ln \left(\frac{f_{n + 1}}{f_{n + 1} \bigr|_{t = 0}}  \right) ~ \phi \left(\left(m - \frac{s^2 \eta_{n + 1, 0}^2}{2} \right) T_{n + 1}, s^2 \eta_{n + 1, 0}^2 T_{n + 1} \right) -->
<!-- \end{equation} -->

<!-- That is, $f_{n + 1}$ is a geometric Brownian motion. Since $f_{n + 1}$ is a geometric Brownian motion, then the same steps in equations \ref{eq:gbmNproof1} to \ref{eq:gbmNproof3} may be followed to show that a function $f_{n + 2}$ of $f_{n + 1}$ is also a geometric Brownian motion, and so on for a function $f_{n + 3}$ of $f_{n + 2}$, ad infinitum. Equations \ref{eq:gBmN} and \ref{eq:bsPDEn} therefore extend to $n > 2$. -->

<!-- Since $f_n$ is a geometric Brownian motion, then substituting $f_n$ for $q$ in equation \ref{eq:rovRaw} gives the following formula for the N-fold real option value, i.e., the option value of an underlying option value (which may itself be the option value of an option value). -->

<!-- \begin{equation} -->
<!-- f_n = e^{(m - r) T_n} (f_{n - 1} \Phi(\delta_n + \eta_{n - 1, 0} s \sqrt{T_n}) - e^{-r T_n} K_n) -->
<!-- \end{equation} -->

<!-- Where -->

<!-- \begin{equation} -->
<!-- \delta_n = \frac{\ln \left( \frac{f_{n - 1}}{K_n} \right) + \left( m - \frac{s^2 \eta_{n - 1, 0}^2}{2} \right) T_n}{s \eta_{n - 1, 0} \sqrt{T_n}} -->
<!-- \end{equation} -->

<!-- And -->

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \eta_{n - 1, 0} &= \frac{\partial \ln(f_{n - 1})}{\partial \ln(f_0)} = \frac{f_0}{f_{n - 1}} \frac{\partial f_{n - 1}}{\partial f_0} \\ -->
<!-- &= e^{(m - r) T_{n - 1}} \frac{f_0}{f_{n - 1}} \Phi_{{n - 1}, 1} -->
<!-- \end{split} -->
<!-- \end{equation} -->

<!-- By Ito's Lemma, the evolution of an $n$-fold option may be expressed -->

<!-- \begin{equation} -->
<!-- \Delta f_n = \left( \frac{\partial f_n}{\partial x} m x + \frac{\partial f_n}{\partial t} + \frac{1}{2} \frac{\partial^2 f_n}{\partial x^2} s^2 x^2 \right) \Delta t + \frac{\partial f_n}{\partial x} s x \epsilon \sqrt{\Delta t} -->
<!-- \label{eq:itoLemN} -->
<!-- \end{equation} -->

<!-- As in the previous section, the random term in this expression may be eliminated by subtracting $\partial f_n / \partial x \Delta x$ from $\Delta f_n$. That is, -->

<!-- \begin{equation} -->
<!-- \Delta f_n - \frac{\partial f_n}{\partial x} \Delta x = \left(\frac{\partial f_n}{\partial t} + \frac{1}{2} \frac{\partial^2 f_n}{\partial x^2} s^2 x^2 \right) \Delta t -->
<!-- \end{equation} -->

<!-- Which, as above, trivially implies -->

<!-- \begin{equation} -->
<!-- \Delta f_n - \frac{\partial f_n}{\partial x} \Delta x = \left( f_n - \frac{\partial f_n}{\partial x} x \right) \kappa \Delta t -->
<!-- \end{equation} -->

<!-- Substituting the righthand side of this equation for the lefthand side of the previous equation then yields the following expression. -->

<!-- \begin{equation} -->
<!-- \kappa f_n - \frac{\partial f_n}{\partial x} x \kappa = \frac{\partial f_n}{\partial t} + \frac{s^2 x^2}{2} \frac{\partial^2 f_n}{\partial x^2} -->
<!-- \label{eq:bsPDEfn} -->
<!-- \end{equation} -->

<!-- Isolating $\kappa f_n$ and letting $\kappa = m$, -->

<!-- \begin{equation} -->
<!-- m f_n = \frac{\partial f_n}{\partial x} x m + \frac{\partial f_n}{\partial t} + \frac{s^2 x^2}{2} \frac{\partial^2 f_n}{\partial x^2} -->
<!-- \label{eq:bsPDEfn} -->
<!-- \end{equation} -->

<!-- But note that the righthand side of this expression is equal to the first term in parenthesis on the righthand side of equation \ref{eq:itoLemN}. Therefore, -->

<!-- \begin{equation} -->
<!-- \Delta f_n = m f_n \Delta t + s x \frac{\partial f_n}{\partial x} \epsilon \sqrt{\Delta t} -->
<!-- \label{eq:itoLemN2} -->
<!-- \end{equation} -->

<!-- The second term on the righthand side can also be reduced as follows. -->

<!-- \begin{equation} -->
<!-- s x \frac{\partial f_n}{\partial x} \epsilon \sqrt{\Delta t} = s f_n \eta_{f, x} \epsilon \sqrt{\Delta t} -->
<!-- \end{equation} -->

<!-- Where $\eta_{n, 0}$ is the elasticity of $f_n$ with respect to the project NPV $x$. That is, -->

<!-- \begin{equation} -->
<!-- \begin{split} -->
<!-- \eta_{n, 0} &= \frac{x}{f_n} \frac{\partial f_n}{\partial x} = \frac{\partial \ln(f_n)}{\partial \ln(x)} \\ -->
<!-- &= \frac{1}{100} \frac{%\Delta f_n}{%\Delta x} -->
<!-- \end{split} -->
<!-- \end{equation} -->

<!-- Equation \ref{eq:itoLemN2} can thus be rewritten -->

<!-- \begin{equation} -->
<!-- \Delta f_n = m f_n \Delta t + s f_n \eta_{n, 0} \epsilon \sqrt{\Delta t} -->
<!-- \label{eq:gbmf} -->
<!-- \end{equation} -->

<!-- By which it follows that $\Delta f_n / f_n$ is normally distributed with mean $m \Delta t$ and variance $s^2 \eta_{n, 0}^2 \Delta t$. From this, it follows that $f_n$ is a geometric Brownian movement, such that $\ln(f_n)$ is normally distributed with mean $\ln(f_n(\hat{t})) + \left(m - \frac{s^2 \eta_{n, 0}^2}{2} \right) \tau_n$ and variance $s^2 \eta_{n, 0}^2 \tau_n$. -->

<!-- \begin{equation} -->
<!-- \ln \left(\frac{f}{f\bigr|_{t = 0}}  \right) ~ \phi \left(\left(m - \frac{s^2 \eta_{f, x}^2}{2} \right) T, s^2 \eta_{f, x}^2 T \right) -->
<!-- \end{equation} -->
<!-- The function $f$ of the geometric Brownian motion $x$ is thus itself a geometric Brownian motion. That is to say, the log returns of $f$ are normally distributed with mean $\left(m - \frac{s^2 \eta_{f, x}^2}{2} \right) T$ and variance $s^2 \eta_{f, x}^2 T$. -->
